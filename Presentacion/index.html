<!DOCTYPE html>
<html lang="es">
<head>
<meta charset="utf-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 1.5.5.dev">
<meta name="author" content="Cloud-DI Team - Departamento de Informática. UAL">
<title>Tame Big Data Hadoop</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/* Asciidoctor default stylesheet | MIT License | http://asciidoctor.org */
/* Remove comment around @import statement below when using as a custom stylesheet */
/*@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";*/
article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section,summary{display:block}
audio,canvas,video{display:inline-block}
audio:not([controls]){display:none;height:0}
[hidden],template{display:none}
script{display:none!important}
html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}
body{margin:0}
a{background:transparent}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
abbr[title]{border-bottom:1px dotted}
b,strong{font-weight:bold}
dfn{font-style:italic}
hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}
input[type="search"]{-webkit-appearance:textfield;-moz-box-sizing:content-box;-webkit-box-sizing:content-box;box-sizing:content-box}
input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,*:before,*:after{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;font-weight:400;font-style:normal;line-height:1;position:relative;cursor:auto}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
body{-webkit-font-smoothing:antialiased}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.center{margin-left:auto;margin-right:auto}
.spread{width:100%}
p.lead,.paragraph.lead>p,#preamble>.sectionbody>.paragraph:first-of-type p{font-size:1.21875em;line-height:1.6}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0;direction:ltr}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:none}
p{font-family:inherit;font-weight:400;font-size:1em;line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #ddddd8;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em;height:0}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{font-size:1em;line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol,ul.no-bullet,ol.no-bullet{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0;font-size:1em}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.no-bullet{list-style:none}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
abbr,acronym{text-transform:uppercase;font-size:90%;color:rgba(0,0,0,.8);border-bottom:1px dotted #ddd;cursor:help}
abbr{text-transform:none}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote cite{display:block;font-size:.9375em;color:rgba(0,0,0,.6)}
blockquote cite:before{content:"\2014 \0020"}
blockquote cite a,blockquote cite a:visited{color:rgba(0,0,0,.6)}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media only screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede}
table thead,table tfoot{background:#f7f8f7;font-weight:bold}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt,table tr:nth-of-type(even){background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{display:table-cell;line-height:1.6}
body{tab-size:4}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.clearfix:before,.clearfix:after,.float-group:before,.float-group:after{content:" ";display:table}
.clearfix:after,.float-group:after{clear:both}
*:not(pre)>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background-color:#f7f7f8;-webkit-border-radius:4px;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre,pre>code{line-height:1.45;color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;text-rendering:optimizeSpeed}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background-color:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menu{color:rgba(0,0,0,.8)}
b.button:before,b.button:after{position:relative;top:-1px;font-weight:400}
b.button:before{content:"[";padding:0 3px 0 2px}
b.button:after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin-left:auto;margin-right:auto;margin-top:0;margin-bottom:0;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header:before,#header:after,#content:before,#content:after,#footnotes:before,#footnotes:after,#footer:before,#footer:after{content:" ";display:table}
#header:after,#content:after,#footnotes:after,#footer:after{clear:both}
#content{margin-top:1.25em}
#content:before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #ddddd8}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #ddddd8;padding-bottom:8px}
#header .details{border-bottom:1px solid #ddddd8;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:-ms-flexbox;display:-webkit-flex;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span:before{content:"\00a0\2013\00a0"}
#header .details br+span.author:before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark:before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber:after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #ddddd8;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #efefed;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media only screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background-color:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #efefed;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #efefed;left:auto;right:0}}
@media only screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:100%;background-color:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:rgba(255,255,255,.8);line-height:1.44}
.sect1{padding-bottom:.625em}
@media only screen and (min-width:768px){.sect1{padding-bottom:1.25em}}
.sect1+.sect1{border-top:1px solid #efefed}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor:before,h2>a.anchor:before,h3>a.anchor:before,#toctitle>a.anchor:before,.sidebarblock>.content>.title>a.anchor:before,h4>a.anchor:before,h5>a.anchor:before,h6>a.anchor:before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock>caption.title{white-space:nowrap;overflow:visible;max-width:0}
.paragraph.lead>p,#preamble>.sectionbody>.paragraph:first-of-type p{color:rgba(0,0,0,.85)}
table.tableblock #preamble>.sectionbody>.paragraph:first-of-type p{font-size:inherit}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #ddddd8;color:rgba(0,0,0,.6)}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border-style:solid;border-width:1px;border-color:#e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;-webkit-border-radius:4px;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock pre:not(.highlight),.listingblock pre[class="highlight"],.listingblock pre[class^="highlight "],.listingblock pre.CodeRay,.listingblock pre.prettyprint{background:#f7f7f8}
.sidebarblock .literalblock pre,.sidebarblock .listingblock pre:not(.highlight),.sidebarblock .listingblock pre[class="highlight"],.sidebarblock .listingblock pre[class^="highlight "],.sidebarblock .listingblock pre.CodeRay,.sidebarblock .listingblock pre.prettyprint{background:#f2f1f1}
.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{-webkit-border-radius:4px;border-radius:4px;word-wrap:break-word;padding:1em;font-size:.8125em}
.literalblock pre.nowrap,.literalblock pre[class].nowrap,.listingblock pre.nowrap,.listingblock pre[class].nowrap{overflow-x:auto;white-space:pre;word-wrap:normal}
@media only screen and (min-width:768px){.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{font-size:.90625em}}
@media only screen and (min-width:1280px){.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{font-size:1em}}
.literalblock.output pre{color:#f7f7f8;background-color:rgba(0,0,0,.9)}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;-webkit-border-radius:4px;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.listingblock>.content{position:relative}
.listingblock code[data-lang]:before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:#999}
.listingblock:hover code[data-lang]:before{display:block}
.listingblock.terminal pre .command:before{content:attr(data-prompt);padding-right:.5em;color:#999}
.listingblock.terminal pre .command:not([data-prompt]):before{content:"$"}
table.pyhltable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.pyhltable td{vertical-align:top;padding-top:0;padding-bottom:0;line-height:1.45}
table.pyhltable td.code{padding-left:.75em;padding-right:0}
pre.pygments .lineno,table.pyhltable td:not(.code){color:#999;padding-left:0;padding-right:.5em;border-right:1px solid #ddddd8}
pre.pygments .lineno{display:inline-block;margin-right:.25em}
table.pyhltable .linenodiv{background:none!important;padding-right:0!important}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock blockquote p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote:before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.5em;margin-right:.5ex;text-align:right}
.quoteblock .quoteblock{margin-left:0;margin-right:0;padding:.5em 0;border-left:3px solid rgba(0,0,0,.6)}
.quoteblock .quoteblock blockquote{padding:0 0 0 .75em}
.quoteblock .quoteblock blockquote:before{display:none}
.verseblock{margin:0 1em 1.25em 1em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract{margin:0 0 1.25em 0;display:block}
.quoteblock.abstract blockquote,.quoteblock.abstract blockquote p{text-align:left;word-spacing:0}
.quoteblock.abstract blockquote:before,.quoteblock.abstract blockquote p:first-of-type:before{display:none}
table.tableblock{max-width:100%;border-collapse:separate}
table.tableblock td>.paragraph:last-child p>p:last-child,table.tableblock th>p:last-child,table.tableblock td>p:last-child{margin-bottom:0}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all th.tableblock,table.grid-all td.tableblock{border-width:0 1px 1px 0}
table.grid-all tfoot>tr>th.tableblock,table.grid-all tfoot>tr>td.tableblock{border-width:1px 1px 0 0}
table.grid-cols th.tableblock,table.grid-cols td.tableblock{border-width:0 1px 0 0}
table.grid-all *>tr>.tableblock:last-child,table.grid-cols *>tr>.tableblock:last-child{border-right-width:0}
table.grid-rows th.tableblock,table.grid-rows td.tableblock{border-width:0 0 1px 0}
table.grid-all tbody>tr:last-child>th.tableblock,table.grid-all tbody>tr:last-child>td.tableblock,table.grid-all thead:last-child>tr>th.tableblock,table.grid-rows tbody>tr:last-child>th.tableblock,table.grid-rows tbody>tr:last-child>td.tableblock,table.grid-rows thead:last-child>tr>th.tableblock{border-bottom-width:0}
table.grid-rows tfoot>tr>th.tableblock,table.grid-rows tfoot>tr>td.tableblock{border-width:1px 0 0 0}
table.frame-all{border-width:1px}
table.frame-sides{border-width:0 1px}
table.frame-topbot{border-width:1px 0}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{display:table-cell;line-height:1.6;background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
td>div.verse{white-space:pre}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
ol>li p,ul>li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.unstyled,ol.unnumbered,ul.checklist,ul.none{list-style-type:none}
ul.unstyled,ol.unnumbered,ul.checklist{margin-left:.625em}
ul.checklist li>p:first-child>.fa-square-o:first-child,ul.checklist li>p:first-child>.fa-check-square-o:first-child{width:1em;font-size:.85em}
ul.checklist li>p:first-child>input[type="checkbox"]:first-child{width:1em;position:relative;top:1px}
ul.inline{margin:0 auto .625em auto;margin-left:-1.375em;margin-right:0;padding:0;list-style:none;overflow:hidden}
ul.inline>li{list-style:none;float:left;margin-left:1.375em;display:block}
ul.inline>li>*{display:block}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist>table tr>td:first-of-type{padding:0 .75em;line-height:1}
.colist>table tr>td:last-of-type{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:solid 4px #fff;-webkit-box-shadow:0 0 0 1px #ddd;box-shadow:0 0 0 1px #ddd}
.imageblock.left,.imageblock[style*="float: left"]{margin:.25em .625em 1.25em 0}
.imageblock.right,.imageblock[style*="float: right"]{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em 0;border-width:1px 0 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;text-indent:-1.05em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
.gist .file-data>table{border:0;background:#fff;width:100%;margin-bottom:0}
.gist .file-data>table td.line-data{width:99%}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background-color:#00fafa}
.black{color:#000}
.black-background{background-color:#000}
.blue{color:#0000bf}
.blue-background{background-color:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background-color:#fa00fa}
.gray{color:#606060}
.gray-background{background-color:#7d7d7d}
.green{color:#006000}
.green-background{background-color:#007d00}
.lime{color:#00bf00}
.lime-background{background-color:#00fa00}
.maroon{color:#600000}
.maroon-background{background-color:#7d0000}
.navy{color:#000060}
.navy-background{background-color:#00007d}
.olive{color:#606000}
.olive-background{background-color:#7d7d00}
.purple{color:#600060}
.purple-background{background-color:#7d007d}
.red{color:#bf0000}
.red-background{background-color:#fa0000}
.silver{color:#909090}
.silver-background{background-color:#bcbcbc}
.teal{color:#006060}
.teal-background{background-color:#007d7d}
.white{color:#bfbfbf}
.white-background{background-color:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background-color:#fafa00}
span.icon>.fa{cursor:default}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note:before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip:before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning:before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution:before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important:before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background-color:rgba(0,0,0,.8);-webkit-border-radius:100px;border-radius:100px;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]:after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background-color:#fffef7;border-color:#e0e0dc;-webkit-box-shadow:0 1px 4px #e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@media print{@page{margin:1.25cm .75cm}
*{-webkit-box-shadow:none!important;box-shadow:none!important;text-shadow:none!important}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare):after,a[href^="https:"]:not(.bare):after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]:after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #ddddd8!important;padding-bottom:0!important}
.sect1{padding-bottom:0!important}
.sect1+.sect1{border:0!important}
#header>h1:first-child{margin-top:1.25rem}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em 0}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span:before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]:before{display:block}
#footer{background:none!important;padding:0 .9375em}
#footer-text{color:rgba(0,0,0,.6)!important;font-size:.9em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css">
</head>
<body class="book toc2 toc-right">
<div id="header">
<h1>Tame Big Data Hadoop</h1>
<div class="details">
<span id="author" class="author">Cloud-DI Team - Departamento de Informática. UAL</span><br>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Tabla de contenidos</div>
<ul class="sectlevel1">
<li><a href="#trueresumen">Resumen</a></li>
<li><a href="#trueconceptos-b-sicos">1. Conceptos básicos</a>
<ul class="sectlevel2">
<li><a href="#truewhat-is-hadoop">1.1. What is Hadoop</a></li>
<li><a href="#trueoverview-of-the-hadoop-ecosystem">1.2. Overview of the Hadoop Ecosystem</a></li>
</ul>
</li>
<li><a href="#truehdfs">2. HDFS</a></li>
<li><a href="#truecomprendiendo-mapreduce">3. Comprendiendo MapReduce</a>
<ul class="sectlevel2">
<li><a href="#trueel-mapper">3.1. El Mapper</a></li>
<li><a href="#trueel-reducer">3.2. El Reducer</a></li>
<li><a href="#truetodo-junto">3.3. Todo junto</a></li>
<li><a href="#trueinstalaci-n-de-lo-que-necesitamos">3.4. Instalación de lo que necesitamos</a></li>
<li><a href="#truemapreduce-con-varios-pasos">3.5. MapReduce con varios pasos</a></li>
</ul>
</li>
<li><a href="#truepig">4. Pig</a></li>
<li><a href="#truespark">5. Spark</a>
<ul class="sectlevel2">
<li><a href="#truecreaci-n-de-rdd">5.1. Creación de RDD</a></li>
<li><a href="#trueejemplo-de-operaci-n-map">5.2. Ejemplo de operación Map</a></li>
<li><a href="#trueejemplo-encontrar-la-pel-cula-con-la-evaluaci-n-media-m-s-baja">5.3. Ejemplo: Encontrar la película con la evaluación media más baja</a></li>
<li><a href="#truespark-sql">5.4. Spark SQL</a></li>
</ul>
</li>
<li><a href="#truehive">6. Hive</a>
<ul class="sectlevel2">
<li><a href="#trueparticionado">6.1. Particionado</a></li>
</ul>
</li>
<li><a href="#truesqoop">7. Sqoop</a>
<ul class="sectlevel2">
<li><a href="#trueincremental-imports">7.1. Incremental imports</a></li>
<li><a href="#trueexportar-datos-a-mysql">7.2. Exportar datos a MySQL</a></li>
</ul>
</li>
<li><a href="#truehbase">8. HBase</a>
<ul class="sectlevel2">
<li><a href="#trueuso-de-pig-para-la-carga-de-datos-en-hbase">8.1. Uso de Pig para la carga de datos en HBase</a></li>
<li><a href="#trueejemplo-cargar-con-pig-la-tabla-de-usuarios-en-hbase">8.2. Ejemplo: Cargar con Pig la tabla de usuarios en HBase</a></li>
</ul>
</li>
<li><a href="#truecassandra">9. Cassandra</a>
<ul class="sectlevel2">
<li><a href="#trueinstalaci-n-de-cassandra">9.1. Instalación de Cassandra</a></li>
</ul>
</li>
<li><a href="#truelinks">10. Links</a></li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="imageblock">
<div class="content">
<img src="./images/di.png" alt="di.png">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="trueresumen"><a class="anchor" href="#trueresumen"></a>Resumen</h2>
<div class="sectionbody">
<div class="paragraph">
<div class="title">Objetivos</div>
<p>*</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Disponibles los repositorios usados en este seminario:</p>
</div>
<div class="paragraph">
<p>*</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="trueconceptos-b-sicos"><a class="anchor" href="#trueconceptos-b-sicos"></a>1. Conceptos básicos</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Hadoop is a powerful tool for transforming and analyzing massive data sets on a cluster of computers, but it consists of hundreds of distinct technologies and understanding how they fit together can be intimidating. (HBase, Hive, Flume, Kafka, MongoDB, Cassandra, Zookeeper, Spark, Storm, Hortonworks, Cloudera, &#8230;&#8203;)</p>
</div>
<div class="paragraph">
<p>At the end of this course you&#8217;ll understand the major components of the Hadoop Ecosystem and learn how to fit them together.</p>
</div>
<div class="paragraph">
<p>We&#8217;ll import and export data into both relational and non-relational data stores, analyze them with SQL like queries using Hadoop, write real working programs with MapReduce, Pig and Spark and design some real world systems using what we&#8217;ve learned.</p>
</div>
<div class="paragraph">
<p>There are multiple vendors of Hadoop technology. I just find Hortoworks to be the easiest one to get up and running with. Cloudera is also very popular, I should point out. Hortonworks and Cloudera are in competition with each other.</p>
</div>
<div class="paragraph">
<p>Well basically, it&#8217;s a pre-installed Hadoop environment that also has a bunch of associate technologies installed as well.</p>
</div>
<div class="paragraph">
<p>Usaremos HDP® 2.5: Ready for the enterprise
Automated (with Ambari 2.4) (<a href="https://es.hortonworks.com/downloads/#sandbox" class="bare">https://es.hortonworks.com/downloads/#sandbox</a>)</p>
</div>
<div class="ulist">
<ul>
<li>
<p>HDP: 127.0.0.1:8888</p>
</li>
<li>
<p>Ambari: 127.0.0.1:8080</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Let&#8217;s download some movie data. So, if you go to your browser and go to grouplens.org, this will take you to some free data that contains some real movie ratings by real people that we can play with. MovieLens 100K Dataset (<a href="http://grouplens.org/datasets/movielens/100k/" class="bare">http://grouplens.org/datasets/movielens/100k/</a>)</p>
</div>
<div class="paragraph">
<p>This contains a bunch of files</p>
</div>
<div class="paragraph">
<p>that contain movie ratings data.</p>
</div>
<div class="paragraph">
<p>The u.data file is what contains the actual ratings data. If we open that up in Word Pad or Notepad or something, you can take a look at the format here. Basically, these are four columns of information. The first column is a user ID. The second column is a movie ID and then a rating from a score of one to five and a timestamp.</p>
</div>
<div class="paragraph">
<p>u.item contiene la descripción de las películas</p>
</div>
<div class="paragraph">
<p>Just go to our browser and go to 127.0.0.1:8888. User y pass: maria_dev. And that brings us into Ambari, which let&#8217;s us visualize what&#8217;s going on in our little Hadoop cluster here.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Haddop está instalado en la máquina virtual como un contenedor. Para iniciar una sesión SSH en el contenedor deberemos hacerlo por el puerto 2222 (<code>ssh maria_dev@127.0.0.1 -p 2222</code>)</p>
</div>
<div class="paragraph">
<p>Para poder actualizar paquete hay que iniciar sesión como <code>root (su root)</code> y editar con <code>less</code> el archivo <code>\etc\yum.repos.d\sandbox.repo</code> y hacer <code>enabled = 0</code></p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Now, the beautiful thing is that with Hadoop, you don&#8217;t really have to worry about if it&#8217;s running on one system, like we&#8217;re doing now, or on multiple notes of an entire cluster of computers. All that is generally abstracted for you from Hadoop, so as a programmer or a developer or a analyst, what you do to develop under Hadoop is not going to be a whole lot different if you&#8217;re running on one machine versus an entire cluster of machines.</p>
</div>
<div class="paragraph">
<p>The main difference is how much data you can actually handle</p>
</div>
<div class="paragraph">
<p>So, what we&#8217;re going to do is import that movie ratings data that we downloaded into our Hadoop cluster and we&#8217;re going to use something called Hive to actually allow us to execute sequel queries on top of that data, even though it&#8217;s not really being stored in the relational database.</p>
</div>
<div class="paragraph">
<p>Hive View - Upload Table - Upload from local
Table name:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Ratings (user_id, movie_id, rating, rating_time)</p>
</li>
<li>
<p>Movie_names (movie_id, name, movie_date)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Let&#8217;s write a little standard sequel query and, again, this is kinda cool because we&#8217;re not really on a relational database here. This is just a tool called Hive that let&#8217;s us interact with our data on Hadoop as if it were a relational database.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>SELECT movie_id, COUNT(movie_id) as ratingCount
FROM ratings
GROUP BY movie_id
ORDER BY ratingCount DESC;</pre>
</div>
</div>
<div class="paragraph">
<p>Probar con Visualización SQL, datos, explorer, &#8230;&#8203;</p>
</div>
<div class="sect2">
<h3 id="truewhat-is-hadoop"><a class="anchor" href="#truewhat-is-hadoop"></a>1.1. What is Hadoop</h3>
<div class="paragraph">
<p>Well the definition from Hortonworks which is one of the major vendors of Hadoop platforms is an open source software platform for distributed storage and distributed processing very large data sets on computer clusters built from commodity hardware.</p>
</div>
<div class="paragraph">
<p>Hadoop gives you a way of viewing all of the data distributed across all of the hard drives in your cluster as one single file system. And not only that it&#8217;s also very redundant. Actually doing that all in a parallel manner so it can actually set all the CPU cores on your entire cluster chugging away on that problem in parallel.</p>
</div>
<div class="paragraph">
<p>Hadoop originally was just made for batch processing and the idea was well you know if you need to run some sort of analysis that you can wait a few minutes for the answer Hadoop might be for you. However, there are systems that can also expose the data from Hadoop and it means that can be consumed by web applications or whatever you want at very high transaction rates so it&#8217;s not just for batch processing anymore.</p>
</div>
</div>
<div class="sect2">
<h3 id="trueoverview-of-the-hadoop-ecosystem"><a class="anchor" href="#trueoverview-of-the-hadoop-ecosystem"></a>1.2. Overview of the Hadoop Ecosystem</h3>
<div class="imageblock">
<div class="content">
<img src="./images/CoreHadoopEcosystem.png" alt="CoreHadoopEcosystem.png">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>HDFS: It allows us to distribute the storage of big data across our cluster of computers so it makes all of the hard drives on our cluster look like one giant file system. And not only that it actually maintains redundant copies of that data.</p>
</li>
<li>
<p>YARN (yet another resource negotiator): It is basically the system that manages the resources on your computing cluster. It&#8217;s what decides what gets to run tasks and when. Which ones are available or not available.</p>
</li>
<li>
<p>MapReduce: It is just a programming metaphor or programming model that allows you to process your data across an entire cluster.  Mappers have the ability to transform your data in parallel across your entire computing cluster in a very efficient manner.And reducers are what aggregate that data together.</p>
</li>
<li>
<p>Pig: If you don&#8217;t want to write Java or python map reduce code and you&#8217;re more familiar with a scripting language that has sort of a SQL style syntax Pig is for you.</p>
</li>
<li>
<p>Hive: It solves a similar problem to pig but it really more directly looks like a SQL database so hive is a way of actually taking SQL queries and making this distributed data that&#8217;s just really sitting on your file system somewhere look like a SQL database. You can even connect to it through a shell client or ODBC or what have you. And actually execute SQL queries on the data that&#8217;s stored on your Hadoop cluster even though it&#8217;s not really a relational database under the hood.</p>
</li>
<li>
<p>Ambari (el que usa Hortoworks): It is basically this thing that sits on top of everything and it just gives you a view of the state of your cluster and lets you visualize what&#8217;s running on your cluster What systems are using how much resources and also has some views in it that allow you to actually do things like execute hive queries or import databases into hive or execute Pig queries and things like that.</p>
</li>
<li>
<p>Mesos: It isn&#8217;t really part of Hadoop proper but I&#8217;m including it here because it&#8217;s basically an alternative to yarn. Mesos is another potential way of managing the resources on your cluster.</p>
</li>
<li>
<p>Spark: Análogo a MapReduce. Es más rápido y es para trabajar con datos en memoria. Además, puede hacer Machine Learning, Procesamiento de datos en streaming en tiempo real</p>
</li>
<li>
<p>HBase: Base de datos NoSQL de columnas para exponer los datos del clúster a plataformas transaccionales ¿?? Se puede usar para exponer los datos de MapReduce o Spark a otros sistemas</p>
</li>
<li>
<p>Apache Storm: Para procesamiento de datos en streaming</p>
</li>
<li>
<p>Oozie: Oozie is just a way of scheduling jobs on your cluster.</p>
</li>
<li>
<p>Zookeeper: It&#8217;s basically a technology for coordinating everything on your cluster. So it&#8217;s it&#8217;s the technology that can be used for keeping track of which nodes are up which nodes are down.</p>
</li>
<li>
<p>Data ingestion:</p>
<div class="ulist">
<ul>
<li>
<p>Sqoop: para transferir datos entre bases de datos relacionales y Hadoop</p>
</li>
<li>
<p>Flume: It&#8217;s a way of actually transporting Web logs at a very large scale and very reliably to your cluster. So let&#8217;s say you have a fleet of web servers. Flume can actually listen to the web logs coming in from those web servers in real time and publish them into your cluster in real time for processing by something like storm or spark streaming.</p>
</li>
<li>
<p>Kafka: Kafka solves a similar problem although it&#8217;s a little bit more general purpose. It can basically collect data of any sort from a cluster of PCs from a cluster of web servers or whatever it is and broadcast that into your Hadoop cluster as well.</p>
</li>
</ul>
</div>
</li>
<li>
<p>External Storage: Almacenamiento que introducen datos o guardan resultados de procesamiento del cluster Hadoop</p>
<div class="ulist">
<ul>
<li>
<p>MySQL</p>
</li>
<li>
<p>Cassandra</p>
</li>
<li>
<p>MongoDB</p>
</li>
</ul>
</div>
</li>
<li>
<p>Query Engines</p>
<div class="ulist">
<ul>
<li>
<p>Apache Drill: Permite escribir consultas SQL que funcionan sobre bases de datos NoSQL (Drill supports a variety of NoSQL databases and file systems, including HBase, MongoDB, MapR-DB, HDFS, MapR-FS, Amazon S3, Azure Blob Storage, Google Cloud Storage, Swift, NAS and local files. A single query can join data from multiple datastores. For example, you can join a user profile collection in MongoDB with a directory of event logs in Hadoop.)</p>
</li>
<li>
<p>Hue: Hue is an open source Analytic Workbench for browsing, querying and visualizing data with focus on SQL and Search. También puede definir jobs y workflows.</p>
</li>
<li>
<p>Apache Phoenix: Similar a Apache Drill (proporciona consultas SQL a distintas tecnologías de almacenamiento) pero proporciona ACID y OLTP. So you can actually make your not SQL Hadoop data store look a lot like a relational data store.</p>
</li>
<li>
<p>Presto: Consultas SQL sobre el cluster. Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. Presto allows querying data where it lives, including Hive, Cassandra, relational databases or even proprietary data stores. A single Presto query can combine data from multiple sources, allowing for analytics across your entire organization. (Evita hacer mucho ETL)</p>
</li>
<li>
<p>Zeppelin: Permite crear notebooks para interactuar con el cluster</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="truehdfs"><a class="anchor" href="#truehdfs"></a>2. HDFS</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Divide y distribuye archivos de gran tamaño en bloques de 128 MB de forma predeterminada.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Name node: Guarda dónde está cada bloque de cada archivo</p>
</li>
<li>
<p>Data node: Los que almacenan realmene los datos</p>
</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Se puede montar un cluster HDFS en Linux vía NFS, NFS Gateways en concreto.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>Subir un archivo desde Ambari:</p>
<div class="ulist">
<ul>
<li>
<p>Crear una carpeta (user &#8594; maria_dev &#8594; ml-100k)</p>
</li>
<li>
<p>Subir el archivo de Movielens (u.data, u.item)</p>
</li>
<li>
<p>Renombrarlos</p>
</li>
</ul>
</div>
</li>
<li>
<p>Subir un archivo desde línea de comandos</p>
<div class="ulist">
<ul>
<li>
<p>SSH a maria_dev@127.0.0.1 (puerto 2222)</p>
</li>
<li>
<p>hadoop fs -mkdir ml-100k</p>
</li>
<li>
<p>hadoop fs -copyFromLocal u.data ml-100K/u.data</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="truecomprendiendo-mapreduce"><a class="anchor" href="#truecomprendiendo-mapreduce"></a>3. Comprendiendo MapReduce</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A partir de un archivo que guarda user_id, movie_id, rating y timestamp, obtener la cantidad de películas que ha valorado cada usuario</p>
</div>
<div class="imageblock">
<div class="content">
<img src="./images/MapReduceInAction.png" alt="MapReduceInAction.png">
</div>
</div>
<div class="paragraph">
<p>Otro ejemplo podría ser obtener la cantidad de valoraciones por valoración:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="./images/MapReduceCantidadValoracionesPorValoracion.png" alt="MapReduceCantidadValoracionesPorValoracion.png">
</div>
</div>
<div class="sect2">
<h3 id="trueel-mapper"><a class="anchor" href="#trueel-mapper"></a>3.1. El Mapper</h3>
<div class="imageblock">
<div class="content">
<img src="./images/MapReduceMapper.png" alt="MapReduceMapper.png">
</div>
</div>
<div class="paragraph">
<p>El <em>mapper</em> toma tres parámetros:
* La instancia del objeto en el que está contenido
* No se suele usar en el caso de los mappers
* La entrada (línea) con la que va a trabajar</p>
</div>
<div class="paragraph">
<p>Vamos a extraer 4 variables (userID, movieID, rating, timestam) tras la división de la línea mediante tabuladores.</p>
</div>
</div>
<div class="sect2">
<h3 id="trueel-reducer"><a class="anchor" href="#trueel-reducer"></a>3.2. El Reducer</h3>
<div class="imageblock">
<div class="content">
<img src="./images/MapReduceReducer.png" alt="MapReduceReducer.png">
</div>
</div>
<div class="paragraph">
<p>EL <em>reducer</em> toma tres parámetros:
* La instancia del objeto en el que se está ejecutando
* La función reducer que se llamará una vez para cada clave única
* La lista de valores asociados a la clave</p>
</div>
</div>
<div class="sect2">
<h3 id="truetodo-junto"><a class="anchor" href="#truetodo-junto"></a>3.3. Todo junto</h3>
<div class="imageblock">
<div class="content">
<img src="./images/MapReduceTodoJunto.png" alt="MapReduceTodoJunto.png">
</div>
</div>
<div class="paragraph">
<p>La función <em>steps</em> indica al framework qué funciones se usan para los mappers y los reducers en nuestros trabajos.</p>
</div>
</div>
<div class="sect2">
<h3 id="trueinstalaci-n-de-lo-que-necesitamos"><a class="anchor" href="#trueinstalaci-n-de-lo-que-necesitamos"></a>3.4. Instalación de lo que necesitamos</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>yum install epel-release
yum install python-pip
pip install mrjob
yum install nano
yum install wget
wget http://media.sundog-soft.com/hadoop/ml-100k/u.data
wget http://media.sundog-soft.com/hadoop/RatingsBreakdown.py

python RatingsBreakdown.py u.data</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Para ejecutar con Hadoop</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>python RatingsBreakdown.py -r hadoop --hadoop-streaming-jar /usr/hdp/current/haddop-mapreduce-client/hadoop-streaming.jar u.data</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="truemapreduce-con-varios-pasos"><a class="anchor" href="#truemapreduce-con-varios-pasos"></a>3.5. MapReduce con varios pasos</h3>
<div class="paragraph">
<p>¿Qué ocurre si queremos hacer más de un paso map o un paso reduce? Por ejemplo, queremos devolver las películas mejor valoradas.</p>
</div>
<div class="paragraph">
<p>Habría que hacer MapReduce para obtener la valoración de cada película y luego ordenarlas.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="./images/MapReduceMultiplesPasos.png" alt="MapReduceMultiplesPasos.png">
</div>
</div>
<div class="paragraph">
<p>Primero, nos aseguramos que los valores tienen cinco caracteres y se completan con ceros a la izquierda para que la ordenación de alfanumérica sea correcta</p>
</div>
<div class="imageblock">
<div class="content">
<img src="./images/MapReduceObtenerNumeros.png" alt="MapReduceObtenerNumeros.png">
</div>
</div>
<div class="paragraph">
<p>Después, obtendríamos la lista ordenada</p>
</div>
<div class="imageblock">
<div class="content">
<img src="./images/MapReduceSalidaOrdenada.png" alt="MapReduceSalidaOrdenada.png">
</div>
</div>
<div class="paragraph">
<p>La solución sería:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="./images/MapReducePeliculasConMasValoraciones.png" alt="MapReducePeliculasConMasValoraciones.png">
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>from mrjob.job import MRJob
from mrjob.step import MRStep

class RatingsBreakdown(MRJob):
    def steps(self):
        return [
            MRStep(mapper=self.mapper_get_ratings,
                   reducer=self.reducer_count_ratings),
            MRStep(reducer=self.reducer_sorted_output)
        ]

    def mapper_get_ratings(self, _, line):
        (userID, movieID, rating, timestamp) = line.split('\t')
        yield movieID, 1

    def reducer_count_ratings(self, key, values):
        yield str(sum(values)).zfill(5), key

    def reducer_sorted_output(self, count, movies):
        for movie in movies:
            yield movie, count


if __name__ == '__main__':
    RatingsBreakdown.run()</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="truepig"><a class="anchor" href="#truepig"></a>4. Pig</h2>
<div class="sectionbody">
<div class="paragraph">
<p>La creación de mappers y reducers a mano es una tarea pesada.</p>
</div>
<div class="paragraph">
<p>Pig introduce <em>Pig Latin</em>, un lenguaje de scripting con sintaxis tipo SQL para definir pasos map y reduce. Además, se puede extender fácilmente con funciones definidas por usuario (UDF)</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>El uso de Pig no implica una penalización en rendimiento, sino todo lo contrario, ya que se ejecuta sobre Tez, un optimizador que hará que el código Pig se ejecute más rápido que si escribimos nuestro programa MapReduce directamente en Java o Python.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Ejemplo: Obtener las películas más antiguas con 5 estrellas</p>
</div>
<div class="imageblock">
<div class="content">
<img src="./images/PigPeliculasAntiguasCon5Estrellas.png" alt="PigPeliculasAntiguasCon5Estrellas.png">
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>ratings = LOAD '/user/maria_dev/ml-100k/rating' AS (userID:int, movieID:int, rating:int, ratingTime:int);

metadata = LOAD '/user/maria_dev/ml-100k/movie' USING PigStorage('|') AS (movieID:int, movieTitle:chararray, releaseDate:chararray, videoRelease:chararray, imdblink:chararray);

nameLookup = FOREACH metadata GENERATE movieID, movieTitle, ToUnixTime(ToDate(releaseDate, 'dd-MMM-yyyy')) as releaseTime;

ratingsByMovie = GROUP ratings BY movieID;

avgRatings = FOREACH ratingsByMovie GENERATE group AS movieID, AVG(ratings.rating) AS avgRating;

fiveStarMovies = FILTER avgRatings BY avgRating &gt; 4.0;

fiveStarsWithData = JOIN fiveStarMovies BY movieID, nameLookup BY movieID;

oldestFiveStarsWithData = ORDER fiveStarsWithData BY nameLookup::releaseTime;

DUMP oldestFiveStarsWithData;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Como resultado:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>(493,4.15,493,Thin Man, The (1934),-1136073600)
(604,4.012345679012346,604,It Happened One Night (1934),-1136073600)
(615,4.0508474576271185,615,39 Steps, The (1935),-1104537600)
(1203,4.0476190476190474,1203,Top Hat (1935),-1104537600)
(613,4.037037037037037,613,My Man Godfrey (1936),-1073001600)</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Pig tiene gran cantidad de operadores y funciones interesantes (CUBE, COGROUP, &#8230;&#8203;), funciones de agregación SIZE y CONCAT, integración con HBase, y más. Consultar <a href="https://pig.apache.org/docs/latest/basic.html#group" class="bare">https://pig.apache.org/docs/latest/basic.html#group</a></p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Ejemplo: Obtener las películas con calificación inferior a 2 y mayor cantidad de comentarios.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>ratings = LOAD '/user/maria_dev/ml-100k/rating' AS (userID:int, movieID:int, rating:int, ratingTime:int);

metadata = LOAD '/user/maria_dev/ml-100k/movie' USING PigStorage('|') AS (movieID:int, movieTitle:chararray, releaseDate:chararray, videoRelease:chararray, imdblink:chararray);

nameLookup = FOREACH metadata GENERATE movieID, movieTitle;

ratingsByMovie = GROUP ratings BY movieID;

avgRatings = FOREACH ratingsByMovie GENERATE group AS movieID, AVG(ratings.rating) AS avgRating, COUNT(ratings.rating) AS valorations;

badMovies = FILTER avgRatings BY avgRating &lt; 2.0;

badMoviesWithData = JOIN badMovies BY movieID, nameLookup BY movieID;

worstBadMoviesWithData = FOREACH badMoviesWithData GENERATE nameLookup::movieTitle as movieName,
	badMovies::avgRating, badMovies::valorations;

finalResult = ORDER worstBadMoviesWithData BY valorations DESC;

DUMP finalResult;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Esta sería una muestra del resultado final:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>(Leave It to Beaver (1997),1.8409090909090908,44)
(Mortal Kombat: Annihilation (1997),1.9534883720930232,43)
(Crow: City of Angels, The (1996),1.9487179487179487,39)
(Bio-Dome (1996),1.903225806451613,31)
(Barb Wire (1996),1.9333333333333333,30)</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="truespark"><a class="anchor" href="#truespark"></a>5. Spark</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Spark es un motor rápido para el procesamiento de datos a gran escala. Tiene características interesantes como machine learning, análisis de grafos y datos en streaming.</p>
</div>
<div class="paragraph">
<p>Podemos escribir programas en Java, Python, &#8230;&#8203; que realicen manipulaciones y transformaciones complejas. Lo que lo diferencia de otras tecnologías como Pig es el ecosistema tan rico que hay alrededor de Spark.</p>
</div>
<div class="paragraph">
<p>Spark es una solución basada en memoria por lo que no tiene que estar accediendo constantemente a HDFS. También tiene un motor basado en DAG que optimiza los workflows y lo hace muy rápido.</p>
</div>
<div class="paragraph">
<p>A diferencia de MapReduce, en el que hay que pensar las soluciones en términos de mappers y reducers, Spark proporciona un framework, de la misma manera que Pig ofrece el suyo. Portanto, hay que trabajar en construir la solución del problema obviando cómo se van a distribuir los datos en el cluster.</p>
</div>
<div class="paragraph">
<p>Spark se construyen en torno a lo que se denomina <em>Resilient Distributed Database (RDD)</em>. Se trata básicamente de un objeto que representa a un dataset y al que se le pueden aplicar métodos para transformarlo, reducirlo o analizarlo y producir nuevos RDDs.</p>
</div>
<div class="paragraph">
<p>Entre los principales componentes de Spark tenemos:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Con Spark Streaming (componente Spark) ya no estamos limitados a hacer procesamiento batch de datos sino que podemos cargar datos a medida que se producen y analizarlos en la ventana de tiempo que definamos.</p>
</li>
<li>
<p>Spark SQL es una interfaz SQL a Spark.</p>
</li>
<li>
<p>MLLib, una librería de herramientas para Machine Learning y Data Mining aplicables a los datos que tenemos en Spark.</p>
</li>
<li>
<p>GraphX permite el tratamiento de datos relacionados analizando las propiedades del grafo que forman.</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="truecreaci-n-de-rdd"><a class="anchor" href="#truecreaci-n-de-rdd"></a>5.1. Creación de RDD</h3>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Para trabar con Spark necesitamos un SparkContext (sc): Creodo por el driver y es responsable de hacer que el dataset sea resilente y distribuido. Es el que crea los RDDs.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>A partir de archivos externos (sistema de archivos local, HDFS, S3, &#8230;&#8203;)</p>
</li>
<li>
<p>Paralelizando una colección de datos pasados como una lista (Esto es bueno para comenzar practicando)</p>
</li>
<li>
<p>JDBC, Cassandra, HBase, Elasticsearch, CSV, JSON, archivos comprimidos, &#8230;&#8203;</p>
</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>A partir de un SparkContext podemos crear un contexto Hive, que nos permitir tratar con los datos desde Hive en SQL.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="trueejemplo-de-operaci-n-map"><a class="anchor" href="#trueejemplo-de-operaci-n-map"></a>5.2. Ejemplo de operación Map</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>rdd = sc.parallelize(1, 2, 3, 4])
squareRDD = rdd.map(lambda x: x * x) <i class="conum" data-value="1"></i><b>(1)</b>

-- Esto devuelve 1, 4, 9, 16</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Define una función el línea que toma <code>x</code> como parámetro de entrada y devuelve su cuadrado. Es una forma muy compacta de definir la función. A <code>map</code> se le está pasando una función como argumento.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Transformaciones sobre RDD:
* map
* flatmap
* filter
* distinct
* sample
* union, interesect, substract, cartesian</p>
</div>
<div class="paragraph">
<p>Acciones sobre RDD:
* collect
* count
* countByValue
* take
* top
* reduce
* &#8230;&#8203;</p>
</div>
</div>
<div class="sect2">
<h3 id="trueejemplo-encontrar-la-pel-cula-con-la-evaluaci-n-media-m-s-baja"><a class="anchor" href="#trueejemplo-encontrar-la-pel-cula-con-la-evaluaci-n-media-m-s-baja"></a>5.3. Ejemplo: Encontrar la película con la evaluación media más baja</h3>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Entrar en Ambari como Admin y modificar la configuración de los servicios Spark 1 y Spark 2.
Desplegar <em>Advanced spark-log4j-properties</em> y cambiar <code>log4j.rootCategory</code> al valor <code>ERROR.console</code></p>
</div>
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Desde Ambari crear la carpeta ml-100k en maria_dev y subir los archivos u.data y u.info de Movielens</p>
</li>
<li>
<p>Conectar por SSH a la máquina virtual a la cuenta de maria_dev</p>
</li>
<li>
<p>Crear una carpeta en el home de maria_dev ml-100k y descargar la información de las películas <code>wget <a href="http://media.sundog-soft.com/hadoop/ml-100k/u.info" class="bare">http://media.sundog-soft.com/hadoop/ml-100k/u.info</a></code></p>
</li>
<li>
<p>En el home de maria_dev descargar los ejemplos de Spark <code>wget <a href="http://media.sundog-soft.com/hadoop/Spark.zip" class="bare">http://media.sundog-soft.com/hadoop/Spark.zip</a></code></p>
</li>
<li>
<p>Descomprimir el zip</p>
</li>
<li>
<p>Ejecutar el programa Spark <code>spark-submit LowestRatedMovieSpark.py</code></p>
</li>
</ol>
</div>
<div class="paragraph">
<p>El resultado es:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>('3 Ninjas: High Noon At Mega Mountain (1998)', 1.0)
('Beyond Bedlam (1993)', 1.0)
('Power 98 (1995)', 1.0)
('Bloody Child, The (1996)', 1.0)
('Amityville: Dollhouse (1996)', 1.0)
('Babyfever (1994)', 1.0)
('Homage (1995)', 1.0)
('Somebody to Love (1994)', 1.0)
('Crude Oasis, The (1995)', 1.0)
('Every Other Weekend (1990)', 1.0)</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><code>spark-submit</code> crea un entorno Spark para ejecutar el programa en el cluster en lugar de en el sistema de archivos local.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="truespark-sql"><a class="anchor" href="#truespark-sql"></a>5.4. Spark SQL</h3>
<div class="paragraph">
<p>Los RDD no son más que un conjunto de filas. No tienen tipo de datos.</p>
</div>
<div class="paragraph">
<p>Los data frames son una extensión de los RDD que contienen objetos cuyas filas contienen datos estructurados. Con los data frames disponemos de columnas y podremos lanzarles consultas.</p>
</div>
<div class="paragraph">
<p>Al tener columnas, se pueden optimizar las consultas haciendo que las respuestas sean más rápidas. Además, nos podremos comunicar mediante JDBC o Tableau y demás, lo que nos permite trabajar a alto nivel.</p>
</div>
<div class="paragraph">
<p>Los datasets son un término más general que los data frames. Los datasets pueden guardar información con tipo pero necesariamente no tienen por que ser una fila como ocurre con los data frames.</p>
</div>
<div class="sect3">
<h4 id="trueejemplo-las-10-pel-culas-con-valoraci-n-media-m-s-baja"><a class="anchor" href="#trueejemplo-las-10-pel-culas-con-valoraci-n-media-m-s-baja"></a>5.4.1. Ejemplo: Las 10 películas con valoración media más baja</h4>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql import functions

def loadMovieNames():
    movieNames = {}
    with open("ml-100k/u.item") as f:
        for line in f:
            fields = line.split('|')
            movieNames[int(fields[0])] = fields[1]
    return movieNames

def parseInput(line):
    fields = line.split()
    return Row(movieID = int(fields[1]), rating = float(fields[2]))

if __name__ == "__main__":
    # Create a SparkSession (the config bit is only for Windows!)
    spark = SparkSession.builder.appName("PopularMovies").getOrCreate()

    # Load up our movie ID -&gt; name dictionary
    movieNames = loadMovieNames()

    # Get the raw data
    lines = spark.sparkContext.textFile("hdfs:///user/maria_dev/ml-100k/u.data")
    # Convert it to a RDD of Row objects with (movieID, rating)
    movies = lines.map(parseInput)
    # Convert that to a DataFrame
    movieDataset = spark.createDataFrame(movies)

    # Compute average rating for each movieID
    averageRatings = movieDataset.groupBy("movieID").avg("rating")

    # Compute count of ratings for each movieID
    counts = movieDataset.groupBy("movieID").count()

    # Join the two together (We now have movieID, avg(rating), and count columns)
    averagesAndCounts = counts.join(averageRatings, "movieID")

    # Pull the top 10 results
    topTen = averagesAndCounts.orderBy("avg(rating)").take(10)

    # Print them out, converting movie ID's to names as we go.
    for movie in topTen:
        print (movieNames[movie[0]], movie[1], movie[2])

    # Stop the session
    spark.stop()</code></pre>
</div>
</div>
<div class="paragraph">
<p>Para ejecutar esto tenemos que indicar que queremos usar Spark 2. Lo haremos configurando una variable de entorno en la máquina virtual (Antes nos habremos conectado por SSH a la cuenta de <code>maria_dev</code>).</p>
</div>
<div class="paragraph">
<p><code>export SPARK_MAJOR_VERSION=2</code></p>
</div>
<div class="paragraph">
<p>Luego ejecutaremos el programa en la máquina virtual con</p>
</div>
<div class="paragraph">
<p><code>spark-submit LowestRatedMovieDataFrame.py</code></p>
</div>
<div class="paragraph">
<p>El resultado es:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>('Further Gesture, A (1996)', 1, 1.0)
('Falling in Love Again (1980)', 2, 1.0)
('Amityville: Dollhouse (1996)', 3, 1.0)
('Power 98 (1995)', 1, 1.0)
('Low Life, The (1994)', 1, 1.0)
('Careful (1992)', 1, 1.0)
('Lotto Land (1995)', 1, 1.0)
('Hostile Intentions (1994)', 1, 1.0)
('Amityville: A New Generation (1993)', 5, 1.0)
('Touki Bouki (Journey of the Hyena) (1973)', 1, 1.0)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="trueejemplo-las-10-pel-culas-peor-valoradas-por-al-menos-10-personas"><a class="anchor" href="#trueejemplo-las-10-pel-culas-peor-valoradas-por-al-menos-10-personas"></a>5.4.2. Ejemplo: Las 10 películas peor valoradas por al menos 10 personas</h4>
<div class="paragraph">
<p>Con esto conseguimos evitar películas que hayan sido valoradas por muy pocas personas (p.e. 1)</p>
</div>
<div class="paragraph">
<p>Usando RDD:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>from pyspark import SparkConf, SparkContext

# This function just creates a Python "dictionary" we can later
# use to convert movie ID's to movie names while printing out
# the final results.
def loadMovieNames():
    movieNames = {}
    with open("ml-100k/u.item") as f:
        for line in f:
            fields = line.split('|')
            movieNames[int(fields[0])] = fields[1]
    return movieNames

# Take each line of u.data and convert it to (movieID, (rating, 1.0))
# This way we can then add up all the ratings for each movie, and
# the total number of ratings for each movie (which lets us compute the average)
def parseInput(line):
    fields = line.split()
    return (int(fields[1]), (float(fields[2]), 1.0))

if __name__ == "__main__":
    # The main script - create our SparkContext
    conf = SparkConf().setAppName("WorstMovies")
    sc = SparkContext(conf = conf)

    # Load up our movie ID -&gt; movie name lookup table
    movieNames = loadMovieNames()

    # Load up the raw u.data file
    lines = sc.textFile("hdfs:///user/maria_dev/ml-100k/u.data")

    # Convert to (movieID, (rating, 1.0))
    movieRatings = lines.map(parseInput)

    # Reduce to (movieID, (sumOfRatings, totalRatings))
    ratingTotalsAndCount = movieRatings.reduceByKey(lambda movie1, movie2: ( movie1[0] + movie2[0], movie1[1] + movie2[1] ) )

    # Filter out movies rated 10 or fewer times
    popularTotalsAndCount = ratingTotalsAndCount.filter(lambda x: x[1][1] &gt; 10)

    # Map to (rating, averageRating)
    averageRatings = popularTotalsAndCount.mapValues(lambda totalAndCount : totalAndCount[0] / totalAndCount[1])

    # Sort by average rating
    sortedMovies = averageRatings.sortBy(lambda x: x[1])

    # Take the top 10 results
    results = sortedMovies.take(10)

    # Print them out:
    for result in results:
        print(movieNames[result[0]], result[1])</code></pre>
</div>
</div>
<div class="paragraph">
<p>Con Spark 2:</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>No olvides hacer <code>export SPARK_MAJOR_VERSION=2</code> para poder usar Spark 2.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql import functions

def loadMovieNames():
    movieNames = {}
    with open("ml-100k/u.item") as f:
        for line in f:
            fields = line.split('|')
            movieNames[int(fields[0])] = fields[1]
    return movieNames

def parseInput(line):
    fields = line.split()
    return Row(movieID = int(fields[1]), rating = float(fields[2]))

if __name__ == "__main__":
    # Create a SparkSession (the config bit is only for Windows!)
    spark = SparkSession.builder.appName("PopularMovies").getOrCreate()

    # Load up our movie ID -&gt; name dictionary
    movieNames = loadMovieNames()

    # Get the raw data
    lines = spark.sparkContext.textFile("hdfs:///user/maria_dev/ml-100k/u.data")
    # Convert it to a RDD of Row objects with (movieID, rating)
    movies = lines.map(parseInput)
    # Convert that to a DataFrame
    movieDataset = spark.createDataFrame(movies)

    # Compute average rating for each movieID
    averageRatings = movieDataset.groupBy("movieID").avg("rating")

    # Compute count of ratings for each movieID
    counts = movieDataset.groupBy("movieID").count()

    # Join the two together (We now have movieID, avg(rating), and count columns)
    averagesAndCounts = counts.join(averageRatings, "movieID")

    # Filter movies rated 10 or fewer times
    popularAveragesAndCounts = averagesAndCounts.filter("count &gt; 10")

    # Pull the top 10 results
    topTen = popularAveragesAndCounts.orderBy("avg(rating)").take(10)

    # Print them out, converting movie ID's to names as we go.
    for movie in topTen:
        print (movieNames[movie[0]], movie[1], movie[2])

    # Stop the session
    spark.stop()</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="truehive"><a class="anchor" href="#truehive"></a>6. Hive</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Hive nos permite ver un cluster Hadoop como si fuese una base de datos relacional. Con Hive podemos consultar los datos del cluster en SQL.</p>
</div>
<div class="paragraph">
<p>Hive traduce las consultas a trabajos MapReduce o Tez.</p>
</div>
<div class="paragraph">
<p>Limitaciones:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Latencia alta. Las consultas tienen que ser traducidas a MapReduce y los trabajos MapReduce tardan bastante en ponerse en marcha.</p>
</li>
<li>
<p>Los datos debven guardarse de forma desnormalizada para que aumente la velocidad de las consultas</p>
</li>
<li>
<p>Realmente, no es una base de datos por lo que no hay transacciones no permite operaciones de inserción, modificación o eliminación.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Ejemplo: Películas con más valoraciones</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>CREATE VIEW IF NOT EXISTS topRatingMovies AS
SELECT movie_id, COUNT(*) AS ratingCount
FROM rating
GROUP BY movie_id
ORDER BY ratingCount DESC;

SELECT movie_title, ratingCount
FROM movie m JOIN topRatingMovies t ON m.movie_id = t.movie_ID;</code></pre>
</div>
</div>
<div class="sect2">
<h3 id="trueparticionado"><a class="anchor" href="#trueparticionado"></a>6.1. Particionado</h3>
<div class="paragraph">
<p>Es posible crear particiones de una tabla a partir de los valores de una o varias  columnas. Esto hace que se almacene cada partición como un subdirectorio.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>CREATE TABLE page_view(viewTime INT,
userid BIGINT,
page_url STRING,
referrer_url STRING,
friends ARRAY&lt;BIGINT&gt;,
properties MAP&lt;STRING, STRING&gt;)
PARTITIONED BY(dt STRING, country STRING) <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Creará particiones para cada combinación de valores diferentes de <code>dt</code> y <code>country</code>.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Al hacer la consulta Hive seleccionará las particiones adecuadas si se usan las columnas <code>dt</code> y <code>country</code> en la consulta.</p>
</div>
<div class="paragraph">
<p>Ejemplo: Películas mejor valoradas con más de 10 valoraciones</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>CREATE VIEW IF NOT EXISTS topRatingMovies AS
SELECT movie_id, COUNT(*) AS ratingCount, AVG(rating) as ratingAvg
FROM rating
GROUP BY movie_id
ORDER BY ratingAvg DESC;

SELECT movie_title, ratingAvg
FROM movie m JOIN topRatingMovies t ON m.movie_id = t.movie_ID
WHERE ratingCount &gt; 10;</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="truesqoop"><a class="anchor" href="#truesqoop"></a>7. Sqoop</h2>
<div class="sectionbody">
<div class="paragraph">
<p>SQL + Hadoop. Nos permite importar y exportar datos de bases de datos relacionales y Hadoop.</p>
</div>
<div class="paragraph">
<p>Ejemplo: Importar una tabla a Hadoop</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>sqoop import --connect jdbc:mysql://localhost/movielens --table movies <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Importa la tabla <code>movies</code> de la base de datos <code>movielens</code>.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Ejemplo: Importar una tabla a Hive</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>sqoop import --connect jdbc:mysql://localhost/movielens --table movies --hive-import <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>El parámetro <code>--hive-import</code> hace que la importación se lleve a cabo en Hive.</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="trueincremental-imports"><a class="anchor" href="#trueincremental-imports"></a>7.1. Incremental imports</h3>
<div class="paragraph">
<p>Incremental imports are performed by comparing the values in a check column against a reference value for the most recent import. For example, if the <code>--incremental</code> append argument was specified, along with <code>--check-column id</code> and <code>--last-value 100</code>, all rows with <code>id &gt; 100</code> will be imported.</p>
</div>
<div class="paragraph">
<p>Esto nos permite tener sincronizado a Hadoop con la base de datos.</p>
</div>
</div>
<div class="sect2">
<h3 id="trueexportar-datos-a-mysql"><a class="anchor" href="#trueexportar-datos-a-mysql"></a>7.2. Exportar datos a MySQL</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>sqoop export --connect jdbc:mysql://db.example.com/foo \
--table exported_movies \ <i class="conum" data-value="1"></i><b>(1)</b>
--export-dir /apps/hive/warehouse/movie <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Tabla de destino en MySQL. Tiene que existir</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Path en Hadoop que contiene los datos a exportar de Hive</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>La máquina virtual de Hortonworks incorpora un MySQL instalado con password <code>hadoop</code> para el usuario <code>root</code>.</p>
</div>
<div class="paragraph">
<p>Para la versión de HDP 2.6.5 y posteriores hay que configurar el pass del usuario <code>root</code> en MySQL.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="./images/MySQLResetPassword.png" alt="MySQLResetPassword.png">
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Ejemplo: Importar datos a MySQL</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>mysql&gt; create database movielens;
mysql&gt; exit;

[maria_dev@sandbox ~]$ wget http://media.sundog-soft.com/hadoop/movielens.sql <i class="conum" data-value="1"></i><b>(1)</b>

mysql&gt; SET NAMES 'utf8'; <i class="conum" data-value="2"></i><b>(2)</b>
mysql&gt; SET CHARACTER SET 'utf8';

mysql&gt; use movielens;
mysql&gt; source movielens.sql <i class="conum" data-value="3"></i><b>(3)</b>
mysql&gt; show tables;
+---------------------+
| Tables_in_movielens |
+---------------------+
| genres              |
| genres_movies       |
| movies              |
| occupations         |
| ratings             |
| users               |
+---------------------+

mysql&gt; GRANT ALL PRIVILEGES ON movielens.* TO ''@'localhost'; <i class="conum" data-value="4"></i><b>(4)</b>

[maria_dev@sandbox ~]$ sqoop import --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table movies <i class="conum" data-value="5"></i><b>(5)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Script que inicializa unas tablas de usuarios, películas y ratings.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Configurar los juegos de caracteres</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Inicializar las tablas de la base de datos en MySQL</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Abrir todos los permisos en <code>localhost</code> para que podamos exportar desde Sqoop los datos de MySQL a Hadoop</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Importar los datos de MySQL a Hadoop. Los datos estarán en la vista de archivos del usuario <code>maria_dev</code>.</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Para importar los datos en Hive, primero deberíamos borrar el directorio <code>movies</code> de <code>maria_dev</code>, ya que la importación a Hive volverá a crearlo como paso intermedio para volcar los datos en él antes de importarlos a Hive.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>[maria_dev@sandbox ~]$ sqoop import --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table movies --hive-import</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Ejemplo: Exportar datos a MySQL</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>mysql&gt; use movielens;
mysql&gt; CREATE TABLE exported_movies (id INTEGER, title VARCHAR(255), releaseDate DATE); <i class="conum" data-value="1"></i><b>(1)</b>

[maria_dev@sandbox ~]$ sqoop export --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table exported_movies --export-dir /apps/hive/warehouse/movies --input-fields-terminated-by '\0001' <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>La tabla de destino tiene que estar creada antes de hacer la exportación</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>No olvidar indicar <code>--input-fields-terminated-by '\0001'</code></td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="truehbase"><a class="anchor" href="#truehbase"></a>8. HBase</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Especializado en hacer operaciones muy rápidas a nivel de fila y ser muy escalable.</p>
</li>
<li>
<p>Expone una API pero no ofrece un lenguaje de consulta.</p>
</li>
<li>
<p>Es muy bueno para tratar con datos dispersos</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Ejemplo: Crear una tabla de ratings agrupados por usuario.</p>
</div>
<div class="paragraph">
<p>Esto nos permite movernos rápidamente por las películas que ha evaluado un usuario.
Vamos a crear un script en Python que pueble esa tabla HBase.
Podremos crear un servicio que devuelva rápidamente los ratings de un userID.
El servicio atenderá peticiones HTTP para almacenar y recuperar datos a través de una API REST.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="./images/HBaseRatingColumnFamily.png" alt="HBaseRatingColumnFamily.png">
</div>
</div>
<div class="paragraph">
<p>En la imagen la familia de columnas es <em>rating</em> y las columnas son los identificadores de películas (50, 33 y 223).</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Expondremos el servicio a través de un puerto de la máquina virtual de HDP. En Virtual Box configuraremos `Settings | Network | Advanced | Port Forwarding. Añadiremos una regla denominada <strong>HBase REST</strong> que mapeará los puertos <strong>8000</strong>.</p>
</li>
<li>
<p>Iniciar sesión en Ambari cono admin e iniciar el servicio HBase.</p>
</li>
<li>
<p>Iniciar sesión SSH en la máquina virtual</p>
</li>
<li>
<p>Cambiar al usuario <code>root</code> y arrancar el servicio REST de HBase ([root@sandbox maria_dev]# /usr/hdp/current/hbase-master/bin/hbase-daemon.sh start rest -p 8000 --infoport 8001
)</p>
</li>
<li>
<p>Instalar con <code>pip</code> el módulo <code>starbase</code> para poder interactuar con HBase.</p>
</li>
<li>
<p>Ejecutar el script Python que carga la tabla HBase y muestra un ejemplo de valoraciones de dos usuarios (<code>%python &#8230;&#8203;./HBaseExamples.py</code>)</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>from starbase import Connection

c = Connection("127.0.0.1", "8000")

ratings = c.table('ratings')

if (ratings.exists()):
    print("Dropping existing ratings table\n")
    ratings.drop()

ratings.create('rating')

print("Parsing the ml-100k ratings data...\n")
ratingFile = open("ml-100k/u.data", "r")

batch = ratings.batch()

for line in ratingFile:
    (userID, movieID, rating, timestamp) = line.split()
    batch.update(userID, {'rating': {movieID: rating}})

ratingFile.close()

print ("Committing ratings data to HBase via REST service\n")
batch.commit(finalize=True)

print ("Get back ratings for some users...\n")
print ("Ratings for user ID 1:\n")
print (ratings.fetch("1"))
print ("Ratings for user ID 33:\n")
print (ratings.fetch("33"))</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Para finalizar HBase API REST</p>
</div>
<div class="listingblock">
<div class="content">
<pre>[root@sandbox maria_dev]# /usr/hdp/current/hbase-master/bin/hbase-daemon.sh stop</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="trueuso-de-pig-para-la-carga-de-datos-en-hbase"><a class="anchor" href="#trueuso-de-pig-para-la-carga-de-datos-en-hbase"></a>8.1. Uso de Pig para la carga de datos en HBase</h3>
<div class="paragraph">
<p>Usar Python para la carga de datos en HBase queda limitado porque sólo dispone del espacio que le proporciona el disco desde el equipo desde el que se lanza.</p>
</div>
<div class="paragraph">
<p>Ahora estamos interesados en tomar los datos directamente del cluster HDFS y guardarlos en una tabla HBase. Por tanto, cuando hablamos de Big Data necesitaremos de Pig o herramientas similares.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><code>importtsv</code> es una herramienta que permite cargar datos HDFS en una tabla HBase.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Pasos:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Crear la tabla HBase de destino</p>
</li>
<li>
<p>Los datos a importar tienen un valor clave único en la primera columna</p>
</li>
<li>
<p>Emplear <code>USING</code> para almacenar los datos en la tabla HBase</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Al ser HBase transaccional a nivel de fila, en una carga masiva de datos desde Pig se pueden lanzar gran cantidad de mappers en el momento de la carga de datos.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="trueejemplo-cargar-con-pig-la-tabla-de-usuarios-en-hbase"><a class="anchor" href="#trueejemplo-cargar-con-pig-la-tabla-de-usuarios-en-hbase"></a>8.2. Ejemplo: Cargar con Pig la tabla de usuarios en HBase</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Subir a HDFS el archivo de usuarios</p>
</li>
<li>
<p>Crear la tabla HBase</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash" data-lang="bash">[maria_dev@sandbox ~]$ hbase shell

hbase(main):002:0&gt; create 'users','userinfo'  <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Crear una tabla <code>users</code> con una familia de columnas denominada <code>userinfo</code></td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Este sería el script Pig que usaríamos. En el script vemos que el ID es único y es la primera columna. También se ve el mapeo de las columnas del archivo HDFS contra la familia de columnas.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>users = LOAD '/user/maria_dev/ml-100k/u.user'
USING PigStorage('|')
AS (userID:int, age:int, gender:chararray, occupation:chararray, zip:int);

STORE users INTO 'hbase://users'
USING org.apache.pig.backend.hadoop.hbase.HBaseStorage (
'userinfo:age,userinfo:gender,userinfo:occupation,userinfo:zip');</code></pre>
</div>
</div>
<div class="paragraph">
<p>Para ejecutarlo, una vez disponible este script en la máquina virtual ejecutarmos</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>[maria_dev@sandbox HadoopMaterials]$ pig hbase.pig</code></pre>
</div>
</div>
<div class="paragraph">
<p>Para ver los resultados de la carga de la tabla <code>users</code>, desde la shell de HBase (<code>hbase shell</code>):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>hbase(main):004:0&gt; scan users
...
 99                   column=userinfo:age, timestamp=1537097539475, value=20    <i class="conum" data-value="1"></i><b>(1)</b>
 99                   column=userinfo:gender, timestamp=1537097539475, value=M
 99                   column=userinfo:occupation, timestamp=1537097539475, value
                      =student
 99                   column=userinfo:zip, timestamp=1537097539475, value=63129
943 row(s) in 2.4820 seconds</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>El resultado muestra la familia de columnas <code>userinfo</code> del usuario 99. La familia de columnas está formada por cuatro columnas: <code>age</code>, <code>gender</code>, <code>occupation</code> y <code>zip</code>.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Para eliminar la tabla primero hay que desactivarla con <code>disable</code></p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="truecassandra"><a class="anchor" href="#truecassandra"></a>9. Cassandra</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Es un DBMS distribuido que no tiene un punto de fallo único. A diferencia de HBase, no tiene nodo máster (el nodo que guarda qué datos guarda cada nodo). Está concebido para la disponibilidad.
* Su modelo de datos es similar al de HBase/BigTable
* Ofrece CQL, su lenguaje de consulta
* Cassandra proporciona consistencia eventual.</p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="title">El Teorema de CAP, consistencia eventual y consistencia ajustable</div>
<div class="paragraph">
<p>Siguiendo el Teorema de CAP, los problemas de Big Data necesitan Tolerancia a fallos (usamos HDFS y nuestro sistema es distribuido y tolerante a fallos de partida), por lo que tenemos que decidir entre Consistencia y Disponibilidad. Cassandra elige consistencia eventual porque si envías un post a una red social no se cae el mundo si el resto de usuarios no ven el post hasta que no pasen 2 ó 3 segundos, lo que tarde en propagar el cambio por todo el cluster.</p>
</div>
<div class="paragraph">
<p>No obstante, en Cassandra se puede ajustar el número de nodos que tienen que confirmar la recepción de un cambio para que se por correcto. Es lo que se conoce como <em>Consistencia ajustable</em></p>
</div>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="./images/cap-theorem.png" alt="cap theorem.png">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Podemos tener Cassandra operativo independientemente de Hadoop, por ejemplo, sirviendo a un sitio web. No obstante, se puede crear una copia para Hadoop para enriquecerlo con capacidades analíticas (Hive, Sparrk, &#8230;&#8203;)</p>
</li>
<li>
<p>CQL tiene algunas limitaciones respecto a SQL</p>
<div class="ulist">
<ul>
<li>
<p>No hay joins por lo que todos los datos tienen que estar desnormalizados
**Las consultas tienen que ser sobre la clave primaria. No se admiten índices secundarios</p>
</li>
</ul>
</div>
</li>
<li>
<p>Casos de uso de Cassandra y Spark</p>
<div class="ulist">
<ul>
<li>
<p>Analítica de datos almacenados en Cassandra</p>
</li>
<li>
<p>Tranbsformar y guardar datos en Cassandra para uso transaccional</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="trueinstalaci-n-de-cassandra"><a class="anchor" href="#trueinstalaci-n-de-cassandra"></a>9.1. Instalación de Cassandra</h3>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
<div class="paragraph">
<p>La máquina virtual quue estamos usando de Hortoworks contiene un sandbox en un contenedor Docker. Su versión de sistema operativo CentOS requiere Python 2.6. Sin embargo Cassandra necesita Python 2.7.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Conexión por SSH a la máquina virtual a la cuenta de <code>maria_dev</code></p>
</li>
<li>
<p>Cambiar al usuario <code>root</code>.</p>
</li>
<li>
<p><code># yum update</code></p>
</li>
<li>
<p><code># yum install scl-utils</code></p>
</li>
<li>
<p><code># yum install centos-release-scl-rh</code></p>
</li>
<li>
<p><code># yum install python27</code></p>
</li>
<li>
<p><code># scl enable python27 bash</code></p>
</li>
<li>
<p>Crear un archivo <code>/etc/yum.repos.d/datastax.repo</code> con este contenido</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre>[datastax]
name = DataStax Repo for Apache Cassandra
baseurl = http://rpm.datastax.com/community
enabled = 1
gpgcheck = 0</pre>
</div>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><code># yum install dsc30</code></p>
</li>
<li>
<p><code># pip install cqlsh</code></p>
</li>
<li>
<p><code># service cassandra start</code></p>
</li>
<li>
<p><code># cqlsh --cqlversion="3.4.0"</code></p>
</li>
<li>
<p><code>cqlsh&gt; CREATE KEYSPACE movielens WITH replication = {'class':'SimpleStrategy', 'replication_factor':'1'} AND durable_writes = true;</code></p>
</li>
<li>
<p><code>cqlsh&gt; USE movielens;</code></p>
</li>
<li>
<p><code>cqlsh:movielens&gt; CREATE TABLE users(user_id int, age int, gender text, occupation text, zip text, PRIMARY KEY(user_id));</code></p>
</li>
<li>
<p><code>cqlsh:movielens&gt; DESCRIBE TABLE users;</code></p>
</li>
<li>
<p><code>cqlsh:movielens&gt; SELECT * FROM users;</code><mark>#</mark> Carga de datos en Cassandra desde Spark</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Usaremos datasets en el script por lo que usaremos Spark 2.0.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code># export SPARK_MAJOR_VERSION=2</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Archivo <code>CassandraSpark.py</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql import functions

def parseInput(line):
    fields = line.split('|')
    return Row(user_id = int(fields[0]), age = int(fields[1]), gender = fields[2], occupation = fields[3], zip = fields[4])

if __name__ == "__main__":
    # Create a SparkSession
    spark = SparkSession.builder.appName("CassandraIntegration").config("spark.cassandra.connection.host", "127.0.0.1").getOrCreate()

    # Get the raw data
    lines = spark.sparkContext.textFile("hdfs:///user/maria_dev/ml-100k/u.user")
    # Convert it to a RDD of Row objects with (userID, age, gender, occupation, zip)
    users = lines.map(parseInput)
    # Convert that to a DataFrame
    usersDataset = spark.createDataFrame(users)

    # Write it into Cassandra
    usersDataset.write\
        .format("org.apache.spark.sql.cassandra")\
        .mode('append')\
        .options(table="users", keyspace="movielens")\
        .save()

    # Read it back from Cassandra into a new Dataframe
    readUsers = spark.read\
    .format("org.apache.spark.sql.cassandra")\
    .options(table="users", keyspace="movielens")\
    .load()

    readUsers.createOrReplaceTempView("users")

    sqlDF = spark.sql("SELECT * FROM users WHERE age &lt; 20")
    sqlDF.show()

    # Stop the session
    spark.stop()</code></pre>
</div>
</div>
<div class="paragraph">
<p>Para ejecutar el archivo:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>spark-submit --packages datastax:spark-cassandra-connector:2.0.0-M2-s_2.11 CassandraSpark.py <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Indicamos la versión 2.0.0 de del conector compatible con Spark 2.0 y Scala 2.11. Estos parámetros se ajustarán a la versión con la que estemos ejecutando.</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="truelinks"><a class="anchor" href="#truelinks"></a>10. Links</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Máquina virtual de Hortoworks: <a href="https://es.hortonworks.com/downloads/#sandbox" class="bare">https://es.hortonworks.com/downloads/#sandbox</a></p>
</li>
<li>
<p>Datos de MovieLens: <a href="https://grouplens.org/datasets/movielens/100k/" class="bare">https://grouplens.org/datasets/movielens/100k/</a></p>
</li>
<li>
<p>Material: <a href="https://sundog-education.com/hadoop-materials/" class="bare">https://sundog-education.com/hadoop-materials/</a></p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2018-09-19 11:54:16 CEST
</div>
</div>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.9.1/styles/github.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.9.1/highlight.min.js"></script>
<script>hljs.initHighlighting()</script>
</body>
</html>