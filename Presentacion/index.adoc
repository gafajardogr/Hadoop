////
NO CAMBIAR!!
Codificación, idioma, tabla de contenidos, tipo de documento
////
:encoding: utf-8
:lang: es
:toc: right
:toc-title: Tabla de contenidos
:doctype: book
:imagesdir: ./images




////
Nombre y título del trabajo
////
# Tame Big Data Hadoop
Cloud-DI Team - Departamento de Informática. UAL

image::di.png[]

// NO CAMBIAR!! (Entrar en modo no numerado de apartados)
:numbered!: 


[abstract]
== Resumen
////
COLOCA A CONTINUACION EL RESUMEN
////



////
COLOCA A CONTINUACION LOS OBJETIVOS
////
.Objetivos
* 

[TIP]
====
Disponibles los repositorios usados en este seminario:

* 
====
// Entrar en modo numerado de apartados
:numbered:

## Conceptos básicos

Hadoop is a powerful tool for transforming and analyzing massive data sets on a cluster of computers, but it consists of hundreds of distinct technologies and understanding how they fit together can be intimidating. (HBase, Hive, Flume, Kafka, MongoDB, Cassandra, Zookeeper, Spark, Storm, Hortonworks, Cloudera, ...)

At the end of this course you'll understand the major components of the Hadoop Ecosystem and learn how to fit them together.

We'll import and export data into both relational and non-relational data stores, analyze them with SQL like queries using Hadoop, write real working programs with MapReduce, Pig and Spark and design some real world systems using what we've learned.

There are multiple vendors of Hadoop technology. I just find Hortoworks to be the easiest one to get up and running with. Cloudera is also very popular, I should point out. Hortonworks and Cloudera are in competition with each other.

Well basically, it's a pre-installed Hadoop environment that also has a bunch of associate technologies installed as well.

Usaremos HDP® 2.5: Ready for the enterprise
Automated (with Ambari 2.4) (https://es.hortonworks.com/downloads/#sandbox)

* HDP: 127.0.0.1:8888
* Ambari: 127.0.0.1:8080 (User y pass: `maria_dev`)

Let's download some movie data. So, if you go to your browser and go to grouplens.org, this will take you to some free data that contains some real movie ratings by real people that we can play with. MovieLens 100K Dataset (http://grouplens.org/datasets/movielens/100k/)

This contains a bunch of files that contain movie ratings data.

The `u.data`` file is what contains the actual ratings data. If we open that up in Word Pad or Notepad or something, you can take a look at the format here. Basically, these are four columns of information. The first column is a user ID. The second column is a movie ID and then a rating from a score of one to five and a timestamp.

`u.item` contiene la descripción de las películas.

Just go to our browser and go to 127.0.0.1:8888. That brings us into Ambari, which let's us visualize what's going on in our little Hadoop cluster here.

[IMPORTANT]
====
Hadoop está instalado en la máquina virtual como un contenedor. Para iniciar una sesión SSH en el contenedor deberemos hacerlo por el puerto 2222 (`ssh maria_dev@127.0.0.1 -p 2222`)

Para poder actualizar paquetes hay que iniciar sesión como `root (su root)` y editar con `less` el archivo `\etc\yum.repos.d\sandbox.repo` y hacer `enabled = 0`
====

Now, the beautiful thing is that with Hadoop, you don't really have to worry about if it's running on one system, like we're doing now, or on multiple notes of an entire cluster of computers. All that is generally abstracted for you from Hadoop, so as a programmer or a developer or a analyst, what you do to develop under Hadoop is not going to be a whole lot different if you're running on one machine versus an entire cluster of machines.

The main difference is how much data you can actually handle

So, what we're going to do is import that movie ratings data that we downloaded into our Hadoop cluster and we're going to use something called Hive to actually allow us to execute SQL queries on top of that data, even though it's not really being stored in the relational database.

[source]
----
Hive View - Upload Table - Upload from local
Table name: 

* Ratings (user_id, movie_id, rating, rating_time)
* Movie_names (movie_id, name, movie_date)
----

Let's write a little standard SQL query and, again, this is kinda cool because we're not really on a relational database here. This is just a tool called Hive that let's us interact with our data on Hadoop as if it were a relational database.

----
SELECT movie_id, COUNT(movie_id) as ratingCount
FROM ratings
GROUP BY movie_id
ORDER BY ratingCount DESC;
----

Probar con Visualización SQL, datos, explorer, ...

### What is Hadoop

Well, the definition from Hortonworks, which is one of the major vendors of Hadoop platforms, is an open source software platform for distributed storage and distributed processing very large data sets on computer clusters built from commodity hardware.

Hadoop gives you a way of viewing all of the data distributed across all of the hard drives in your cluster as one single file system. And not only that it's also very redundant. Actually doing that all in a parallel manner so it can actually set all the CPU cores on your entire cluster chugging away on that problem in parallel.

Hadoop originally was just made for batch processing and the idea was well you know if you need to run some sort of analysis that you can wait a few minutes for the answer Hadoop might be for you. However, there are systems that can also expose the data from Hadoop and it means that can be consumed by web applications or whatever you want at very high transaction rates so it's not just for batch processing anymore.

### Overview of the Hadoop Ecosystem

image::./CoreHadoopEcosystem.png[]

* HDFS: It allows us to distribute the storage of big data across our cluster of computers so it makes all of the hard drives on our cluster look like one giant file system. And not only that it actually maintains redundant copies of that data.
* YARN (yet another resource negotiator): It is basically the system that manages the resources on your computing cluster. It's what decides what gets to run tasks and when. Which ones are available or not available.
* MapReduce: It is just a programming metaphor or programming model that allows you to process your data across an entire cluster.  Mappers have the ability to transform your data in parallel across your entire computing cluster in a very efficient manner.And reducers are what aggregate that data together.
* Pig: If you don't want to write Java or Python MapReduce code and you're more familiar with a scripting language that has sort of a SQL style syntax Pig is for you.
* Hive: It solves a similar problem to Pig but it really more directly looks like a SQL database. So, Hive is a way of actually taking SQL queries and making this distributed data that's just really sitting on your file system somewhere look like a SQL database. You can even connect to it through a shell client or ODBC or what have you. And actually execute SQL queries on the data that's stored on your Hadoop cluster even though it's not really a relational database under the hood.
* Ambari (el que usa Hortoworks): It is basically this thing that sits on top of everything and it just gives you a view of the state of your cluster and lets you visualize what's running on your cluster What systems are using how much resources and also has some views in it that allow you to actually do things like execute hive queries or import databases into hive or execute Pig queries and things like that.
* Mesos: It isn't really part of Hadoop proper but I'm including it here because it's basically an alternative to YARN. Mesos is another potential way of managing the resources on your cluster.
* Spark: Análogo a MapReduce. Es más rápido y es para trabajar con datos en memoria. Además, puede hacer Machine Learning, Procesamiento de datos en streaming en tiempo real
* HBase: Base de datos NoSQL de columnas para exponer los datos del clúster a plataformas transaccionales. Tiene gran cantidad de conectores que nos permite cargar o volcar datos a otras BD. Se puede usar para exponer los datos de MapReduce o Spark a otros sistemas
* Apache Storm: Para procesamiento de datos en streaming
* Oozie: Oozie is just a way of scheduling jobs on your cluster.
* Zookeeper: It's basically a technology for coordinating everything on your cluster. So it's it's the technology that can be used for keeping track of which nodes are up which nodes are down.
* Data ingestion:
    ** Sqoop: Para transferir datos entre bases de datos relacionales y Hadoop
    ** Flume: It's a way of actually transporting Web logs at a very large scale and very reliably to your cluster. So let's say you have a fleet of web servers. Flume can actually listen to the web logs coming in from those web servers in real time and publish them into your cluster in real time for processing by something like Storm or Spark streaming.
    ** Kafka: Kafka solves a similar problem although it's a little bit more general purpose. It can basically collect data of any sort from a cluster of PCs from a cluster of web servers or whatever it is and broadcast that into your Hadoop cluster as well.
* External Storage: Almacenamiento que introducen datos o guardan resultados de procesamiento del cluster Hadoop
    ** MySQL
    ** Cassandra
    ** MongoDB
* Query Engines
    ** Apache Drill: Permite escribir consultas SQL que funcionan sobre bases de datos NoSQL (Drill supports a variety of NoSQL databases and file systems, including HBase, MongoDB, MapR-DB, HDFS, MapR-FS, Amazon S3, Azure Blob Storage, Google Cloud Storage, Swift, NAS and local files. A single query can join data from multiple datastores. For example, you can join a user profile collection in MongoDB with a directory of event logs in Hadoop.)
    ** Hue: Hue is an open source Analytic Workbench for browsing, querying and visualizing data with focus on SQL and Search. También puede definir jobs y workflows.
    ** Apache Phoenix: Similar a Apache Drill (proporciona consultas SQL a distintas tecnologías de almacenamiento) pero proporciona ACID y OLTP. So you can actually make your not SQL Hadoop data store look a lot like a relational data store.
    ** Presto: Consultas SQL sobre el cluster. Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. Presto allows querying data where it lives, including Hive, Cassandra, relational databases or even proprietary data stores. A single Presto query can combine data from multiple sources, allowing for analytics across your entire organization. (Evita hacer mucho ETL)
    ** Zeppelin: Permite crear notebooks para interactuar con el cluster

## HDFS

Divide y distribuye archivos de gran tamaño en bloques de 128 MB de forma predeterminada.

* Name node: Guarda dónde está cada bloque de cada archivo
* Data node: Los que almacenan realmente los datos

[TIP]
====
Se puede montar un cluster HDFS en Linux vía NFS, NFS Gateways en concreto.
====

* Subir un archivo desde Ambari: 
    ** Crear una carpeta (`/user/maria_dev/ml-100k``)
    ** Subir los archivos de Movielens (`u.data, u.item`)
    ** Renombrarlos y volverlos a dejar con el mismo nombre
* Subir un archivo desde línea de comandos
    ** SSH a `maria_dev@127.0.0.1` (puerto 2222)
    ** `hadoop fs -mkdir ml-100k`
    ** `hadoop fs -copyFromLocal u.data ml-100K/u.data`
    
## Comprendiendo MapReduce

A partir de un archivo que guarda _user_id, movie_id, rating_ y _timestamp_, obtener la cantidad de películas que ha valorado cada usuario

image::MapReduceInAction.png[]

[TIP]
====
Piensa en la operación _Shuffle and sort_ como un `GROUP BY` de SQL y en la operación _Reduce_ como las funciones de agregación (`COUNT, SUM, AVG, ...`).
====

Otro ejemplo podría ser obtener la cantidad de valoraciones por valoración:

image::./MapReduceCantidadValoracionesPorValoracion.png[]

### El Mapper

image::MapReduceMapper.png[]

El _mapper_ toma tres parámetros:

. La instancia del objeto en el que está contenido
. No se suele usar en el caso de los mappers
. La entrada (línea) con la que va a trabajar

Vamos a extraer 4 variables (userID, movieID, rating, timestamp) tras la división de la línea mediante tabuladores.

### El Reducer

image::MapReduceReducer.png[]

El _reducer_ toma tres parámetros:

. La instancia del objeto en el que se está ejecutando
. La función reducer que se llamará una vez para cada clave única????
. La lista de valores asociados a la clave

### Todo junto

image::MapReduceTodoJunto.png[]

La función _steps_ indica al framework qué funciones se usan para los mappers y los reducers en nuestros trabajos.

### Instalación de lo que necesitamos

Recordar que para poder actualizar paquetes hay que iniciar sesión como root (su root) y editar con less el archivo \etc\yum.repos.d\sandbox.repo y hacer enabled = 0

[source]
----
yum install epel-release
yum install python-pip
pip install mrjob
yum install nano
yum install wget
wget http://media.sundog-soft.com/hadoop/ml-100k/u.data
wget http://media.sundog-soft.com/hadoop/RatingsBreakdown.py

python RatingsBreakdown.py u.data
----

[NOTE]
====
Para ejecutar con Hadoop

[source]
----
python RatingsBreakdown.py -r hadoop --hadoop-streaming-jar /usr/hdp/current/haddop-mapreduce-client/hadoop-streaming.jar u.data
----
====

### MapReduce con varios pasos

¿Qué ocurre si queremos hacer más de un paso Map o un paso Reduce? Por ejemplo, queremos devolver las películas mejor valoradas. 

Habría que hacer MapReduce para obtener la valoración de cada película y luego ordenarlas.

image::MapReduceMultiplesPasos.png[]

Primero, nos aseguramos que los valores tienen cinco caracteres y se completan con ceros a la izquierda para que la ordenación de alfanumérica sea correcta

image::MapReduceObtenerNumeros.png[]

Después, obtendríamos la lista ordenada

image::MapReduceSalidaOrdenada.png[]

La solución sería:

image::MapReducePeliculasConMasValoraciones.png[]

[source]
----
from mrjob.job import MRJob
from mrjob.step import MRStep

class RatingsBreakdown(MRJob):
    def steps(self):
        return [
            MRStep(mapper=self.mapper_get_ratings,
                   reducer=self.reducer_count_ratings),
            MRStep(reducer=self.reducer_sorted_output)
        ]

    def mapper_get_ratings(self, _, line):
        (userID, movieID, rating, timestamp) = line.split('\t')
        yield movieID, 1

    def reducer_count_ratings(self, key, values):
        yield str(sum(values)).zfill(5), key

    def reducer_sorted_output(self, count, movies):
        for movie in movies:
            yield movie, count


if __name__ == '__main__':
    RatingsBreakdown.run()
----

## Pig

La creación de mappers y reducers a mano es una tarea pesada.

Pig introduce _Pig Latin_, un lenguaje de scripting con sintaxis tipo SQL para definir pasos Map y Reduce. Además, se puede extender fácilmente con funciones definidas por usuario (UDF)

[NOTE]
====
El uso de Pig no implica una penalización en rendimiento, sino todo lo contrario, ya que se ejecuta sobre Tez, un optimizador que hará que el código Pig se ejecute más rápido que si escribimos nuestro programa MapReduce directamente en Java o Python.
====

Ejemplo: Obtener las películas más antiguas con 5 estrellas

[source]
----
ratings = LOAD '/user/maria_dev/ml-100k/rating' AS (userID:int, movieID:int, rating:int, ratingTime:int);

metadata = LOAD '/user/maria_dev/ml-100k/movie' USING PigStorage('|') AS (movieID:int, movieTitle:chararray, releaseDate:chararray, videoRelease:chararray, imdblink:chararray);

nameLookup = FOREACH metadata GENERATE movieID, movieTitle, ToUnixTime(ToDate(releaseDate, 'dd-MMM-yyyy')) as releaseTime;

ratingsByMovie = GROUP ratings BY movieID;

avgRatings = FOREACH ratingsByMovie GENERATE group AS movieID, AVG(ratings.rating) AS avgRating;

fiveStarMovies = FILTER avgRatings BY avgRating > 4.0;

fiveStarsWithData = JOIN fiveStarMovies BY movieID, nameLookup BY movieID;

oldestFiveStarsWithData = ORDER fiveStarsWithData BY nameLookup::releaseTime;

DUMP oldestFiveStarsWithData;
----

Como resultado:

[source]
----
(493,4.15,493,Thin Man, The (1934),-1136073600)
(604,4.012345679012346,604,It Happened One Night (1934),-1136073600)
(615,4.0508474576271185,615,39 Steps, The (1935),-1104537600)
(1203,4.0476190476190474,1203,Top Hat (1935),-1104537600)
(613,4.037037037037037,613,My Man Godfrey (1936),-1073001600)
----

[TIP]
====
Pig tiene gran cantidad de operadores y funciones interesantes (`CUBE, COGROUP`, ...), funciones de agregación `SIZE` y `CONCAT`, integración con HBase, y más. Consultar https://pig.apache.org/docs/latest/basic.html#group
====

Ejemplo: Obtener las películas con calificación inferior a 2 y mayor cantidad de comentarios.

[source]
----
ratings = LOAD '/user/maria_dev/ml-100k/rating' AS (userID:int, movieID:int, rating:int, ratingTime:int);

metadata = LOAD '/user/maria_dev/ml-100k/movie' USING PigStorage('|') AS (movieID:int, movieTitle:chararray, releaseDate:chararray, videoRelease:chararray, imdblink:chararray);

nameLookup = FOREACH metadata GENERATE movieID, movieTitle;

ratingsByMovie = GROUP ratings BY movieID;

avgRatings = FOREACH ratingsByMovie GENERATE group AS movieID, AVG(ratings.rating) AS avgRating, COUNT(ratings.rating) AS valorations;

badMovies = FILTER avgRatings BY avgRating < 2.0;

badMoviesWithData = JOIN badMovies BY movieID, nameLookup BY movieID;

worstBadMoviesWithData = FOREACH badMoviesWithData GENERATE nameLookup::movieTitle as movieName,
	badMovies::avgRating, badMovies::valorations;

finalResult = ORDER worstBadMoviesWithData BY valorations DESC;

DUMP finalResult;
----

Esta sería una muestra del resultado final:

[source]
----
(Leave It to Beaver (1997),1.8409090909090908,44)
(Mortal Kombat: Annihilation (1997),1.9534883720930232,43)
(Crow: City of Angels, The (1996),1.9487179487179487,39)
(Bio-Dome (1996),1.903225806451613,31)
(Barb Wire (1996),1.9333333333333333,30)
----

## Spark

Spark es un motor rápido para el procesamiento de datos a gran escala. Tiene características interesantes como Machine learning y Análisis de grafos y datos en streaming.

Podemos escribir programas en Java, Python, ... que realicen manipulaciones y transformaciones complejas. Lo que lo diferencia de otras tecnologías como Pig es el ecosistema tan rico que hay alrededor de Spark.

Spark es una solución basada en memoria por lo que no tiene que estar accediendo constantemente a HDFS. También tiene un motor basado en DAG que optimiza los workflows y lo hace muy rápido.

A diferencia de MapReduce, en el que hay que pensar las soluciones en términos de mappers y reducers, Spark proporciona un framework, de la misma manera que Pig ofrece el suyo. Por tanto, hay que trabajar en construir la solución del problema obviando cómo se van a distribuir los datos en el cluster.

Spark se construye en torno a lo que se denomina _Resilient Distributed Database (RDD)_. Se trata básicamente de un objeto que representa a un _dataset_ y al que se le pueden aplicar métodos para transformarlo, reducirlo o analizarlo y producir nuevos RDDs.

Entre los principales componentes de Spark tenemos:

* Con Spark Streaming (componente Spark) ya no estamos limitados a hacer procesamiento batch de datos sino que podemos cargar datos a medida que se producen y analizarlos en la ventana de tiempo que definamos.
* Spark SQL es una interfaz SQL a Spark.
* MLLib, una librería de herramientas para Machine Learning y Data Mining aplicables a los datos que tenemos en Spark.
* GraphX permite el tratamiento de datos relacionados analizando las propiedades del grafo que forman.


### Creación de RDD

[NOTE]
====
Para trabajar con Spark necesitamos un SparkContext (sc): Creado por el driver y es responsable de hacer que el dataset sea resilente y distribuido. Es el que crea los RDDs.
====

* A partir de archivos externos (sistema de archivos local, HDFS, S3, ...)
* Paralelizando una colección de datos pasados como una lista (Esto es bueno para comenzar practicando)
* JDBC, Cassandra, HBase, Elasticsearch, CSV, JSON, archivos comprimidos, ...

[TIP]
====
A partir de un SparkContext podemos crear un contexto Hive, que nos permitir tratar con los datos desde Hive en SQL.
====

### Ejemplo de operación Map

[source]
----
rdd = sc.parallelize([1, 2, 3, 4])
squareRDD = rdd.map(lambda x: x * x) <1>

-- Esto devuelve 1, 4, 9, 16
----
<1> Define una función el línea que toma `x` como parámetro de entrada y devuelve su cuadrado. Es una forma muy compacta de definir la función. A `map` se le está pasando una función como argumento.

Transformaciones sobre RDD:

* map
* flatmap
* filter
* distinct
* sample
* union, interesect, substract, cartesian

Acciones sobre RDD:

* collect
* count
* countByValue
* take
* top
* reduce
* ...

### Ejemplo: Encontrar la película con la evaluación media más baja

[NOTE]
====
Entrar en Ambari como admin y modificar la configuración de los servicios Spark 1 y Spark 2.
Desplegar _Advanced spark-log4j-properties_ y cambiar `log4j.rootCategory` al valor `ERROR.console`
====

. Desde Ambari crear la carpeta `ml-100k` en `maria_dev`` y subir los archivos `u.data` y `u.info` de Movielens
. Conectar por SSH a la máquina virtual a la cuenta de `maria_dev`
. Crear una carpeta en el home de `maria_dev ml-100k` y descargar la información de las películas `wget http://media.sundog-soft.com/hadoop/ml-100k/u.info`
. En el home de `maria_dev` descargar los ejemplos de Spark `wget http://media.sundog-soft.com/hadoop/Spark.zip`
. Descomprimir el zip
. Ejecutar el programa Spark `spark-submit LowestRatedMovieSpark.py`

El resultado es:

[source]
----
('3 Ninjas: High Noon At Mega Mountain (1998)', 1.0)                            
('Beyond Bedlam (1993)', 1.0)
('Power 98 (1995)', 1.0)
('Bloody Child, The (1996)', 1.0)
('Amityville: Dollhouse (1996)', 1.0)
('Babyfever (1994)', 1.0)
('Homage (1995)', 1.0)
('Somebody to Love (1994)', 1.0)
('Crude Oasis, The (1995)', 1.0)
('Every Other Weekend (1990)', 1.0)
----

[NOTE]
====
`spark-submit` crea un entorno Spark para ejecutar el programa en el cluster en lugar de en el sistema de archivos local.
====

### Spark SQL

Los RDD no son más que un conjunto de filas. No tienen tipo de datos.

En cambio, los data frames son una extensión de los RDD que contienen objetos cuyas filas contienen datos estructurados. Con los data frames disponemos de columnas y podremos lanzarles consultas. Al tener columnas, se pueden optimizar las consultas haciendo que las respuestas sean más rápidas. Además, nos podremos comunicar mediante JDBC o Tableau y demás, lo que nos permite trabajar a alto nivel.

Los datasets son un término más general que los data frames. Los datasets pueden guardar información con tipo pero necesariamente no tienen por que ser una fila como ocurre con los data frames.

#### Ejemplo: Las 10 películas con valoración media más baja

[source]
----
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql import functions

def loadMovieNames():
    movieNames = {}
    with open("ml-100k/u.item") as f:
        for line in f:
            fields = line.split('|')
            movieNames[int(fields[0])] = fields[1]
    return movieNames

def parseInput(line):
    fields = line.split()
    return Row(movieID = int(fields[1]), rating = float(fields[2]))

if __name__ == "__main__":
    # Create a SparkSession (the config bit is only for Windows!)
    spark = SparkSession.builder.appName("PopularMovies").getOrCreate()

    # Load up our movie ID -> name dictionary
    movieNames = loadMovieNames()

    # Get the raw data
    lines = spark.sparkContext.textFile("hdfs:///user/maria_dev/ml-100k/u.data")
    # Convert it to a RDD of Row objects with (movieID, rating)
    movies = lines.map(parseInput)
    # Convert that to a DataFrame
    movieDataset = spark.createDataFrame(movies)

    # Compute average rating for each movieID
    averageRatings = movieDataset.groupBy("movieID").avg("rating")

    # Compute count of ratings for each movieID
    counts = movieDataset.groupBy("movieID").count()

    # Join the two together (We now have movieID, avg(rating), and count columns)
    averagesAndCounts = counts.join(averageRatings, "movieID")

    # Pull the top 10 results
    topTen = averagesAndCounts.orderBy("avg(rating)").take(10)

    # Print them out, converting movie ID's to names as we go.
    for movie in topTen:
        print (movieNames[movie[0]], movie[1], movie[2])

    # Stop the session
    spark.stop()
----

Para ejecutar esto tenemos que indicar que queremos usar Spark 2. Lo haremos configurando una variable de entorno en la máquina virtual (Antes nos habremos conectado por SSH a la cuenta de `maria_dev`).

`export SPARK_MAJOR_VERSION=2` 

Luego ejecutaremos el programa en la máquina virtual con 

`spark-submit LowestRatedMovieDataFrame.py`

El resultado es:

[source]
----
('Further Gesture, A (1996)', 1, 1.0)                                           
('Falling in Love Again (1980)', 2, 1.0)
('Amityville: Dollhouse (1996)', 3, 1.0)
('Power 98 (1995)', 1, 1.0)
('Low Life, The (1994)', 1, 1.0)
('Careful (1992)', 1, 1.0)
('Lotto Land (1995)', 1, 1.0)
('Hostile Intentions (1994)', 1, 1.0)
('Amityville: A New Generation (1993)', 5, 1.0)
('Touki Bouki (Journey of the Hyena) (1973)', 1, 1.0)
----

#### Ejemplo: Las 10 películas peor valoradas por al menos 10 personas

Con esto conseguimos evitar películas que hayan sido valoradas por muy pocas personas (p.e. 1)

Usando RDD:

[source]
----
from pyspark import SparkConf, SparkContext

# This function just creates a Python "dictionary" we can later
# use to convert movie ID's to movie names while printing out
# the final results.
def loadMovieNames():
    movieNames = {}
    with open("ml-100k/u.item") as f:
        for line in f:
            fields = line.split('|')
            movieNames[int(fields[0])] = fields[1]
    return movieNames

# Take each line of u.data and convert it to (movieID, (rating, 1.0))
# This way we can then add up all the ratings for each movie, and
# the total number of ratings for each movie (which lets us compute the average)
def parseInput(line):
    fields = line.split()
    return (int(fields[1]), (float(fields[2]), 1.0))

if __name__ == "__main__":
    # The main script - create our SparkContext
    conf = SparkConf().setAppName("WorstMovies")
    sc = SparkContext(conf = conf)

    # Load up our movie ID -> movie name lookup table
    movieNames = loadMovieNames()

    # Load up the raw u.data file
    lines = sc.textFile("hdfs:///user/maria_dev/ml-100k/u.data")

    # Convert to (movieID, (rating, 1.0))
    movieRatings = lines.map(parseInput)

    # Reduce to (movieID, (sumOfRatings, totalRatings))
    ratingTotalsAndCount = movieRatings.reduceByKey(lambda movie1, movie2: ( movie1[0] + movie2[0], movie1[1] + movie2[1] ) )

    # Filter out movies rated 10 or fewer times
    popularTotalsAndCount = ratingTotalsAndCount.filter(lambda x: x[1][1] > 10)

    # Map to (rating, averageRating)
    averageRatings = popularTotalsAndCount.mapValues(lambda totalAndCount : totalAndCount[0] / totalAndCount[1])

    # Sort by average rating
    sortedMovies = averageRatings.sortBy(lambda x: x[1])

    # Take the top 10 results
    results = sortedMovies.take(10)

    # Print them out:
    for result in results:
        print(movieNames[result[0]], result[1])
----

Con Spark 2:

[NOTE]
====
No olvides hacer `export SPARK_MAJOR_VERSION=2` para poder usar Spark 2.
====

[source]
----
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql import functions

def loadMovieNames():
    movieNames = {}
    with open("ml-100k/u.item") as f:
        for line in f:
            fields = line.split('|')
            movieNames[int(fields[0])] = fields[1]
    return movieNames

def parseInput(line):
    fields = line.split()
    return Row(movieID = int(fields[1]), rating = float(fields[2]))

if __name__ == "__main__":
    # Create a SparkSession (the config bit is only for Windows!)
    spark = SparkSession.builder.appName("PopularMovies").getOrCreate()

    # Load up our movie ID -> name dictionary
    movieNames = loadMovieNames()

    # Get the raw data
    lines = spark.sparkContext.textFile("hdfs:///user/maria_dev/ml-100k/u.data")
    # Convert it to a RDD of Row objects with (movieID, rating)
    movies = lines.map(parseInput)
    # Convert that to a DataFrame
    movieDataset = spark.createDataFrame(movies)

    # Compute average rating for each movieID
    averageRatings = movieDataset.groupBy("movieID").avg("rating")

    # Compute count of ratings for each movieID
    counts = movieDataset.groupBy("movieID").count()

    # Join the two together (We now have movieID, avg(rating), and count columns)
    averagesAndCounts = counts.join(averageRatings, "movieID")

    # Filter movies rated 10 or fewer times
    popularAveragesAndCounts = averagesAndCounts.filter("count > 10")

    # Pull the top 10 results
    topTen = popularAveragesAndCounts.orderBy("avg(rating)").take(10)

    # Print them out, converting movie ID's to names as we go.
    for movie in topTen:
        print (movieNames[movie[0]], movie[1], movie[2])

    # Stop the session
    spark.stop()
----

## Hive

Hive nos permite ver un cluster Hadoop como si fuese una base de datos relacional. Con Hive podemos consultar los datos del cluster en SQL.

Hive traduce las consultas a trabajos MapReduce o Tez.

Limitaciones:

* Latencia alta. Las consultas tienen que ser traducidas a MapReduce y los trabajos MapReduce tardan bastante en ponerse en marcha.
* Los datos deben guardarse de forma desnormalizada para que aumente la velocidad de las consultas
* Realmente, no es una base de datos por lo que no hay transacciones y no permite operaciones de inserción, modificación o eliminación.

Ejemplo: Películas con más valoraciones

[source]
----
CREATE VIEW IF NOT EXISTS topRatingMovies AS
SELECT movie_id, COUNT(*) AS ratingCount
FROM rating
GROUP BY movie_id
ORDER BY ratingCount DESC;

SELECT movie_title, ratingCount
FROM movie m JOIN topRatingMovies t ON m.movie_id = t.movie_ID;
----

### Particionado

Es posible crear particiones de una tabla a partir de los valores de una o varias  columnas. Esto hace que se almacene cada partición como un subdirectorio.

[source]
----
CREATE TABLE page_view(viewTime INT, 
userid BIGINT,
page_url STRING, 
referrer_url STRING,
friends ARRAY<BIGINT>, 
properties MAP<STRING, STRING>)
PARTITIONED BY(dt STRING, country STRING) <1>
----
<1> Creará particiones para cada combinación de valores diferentes de `dt` y `country`.

Al hacer la consulta Hive seleccionará las particiones adecuadas si se usan las columnas `dt` y `country` en la consulta.

Ejemplo: Películas mejor valoradas con más de 10 valoraciones

[source]
----
CREATE VIEW IF NOT EXISTS topRatingMovies AS
SELECT movie_id, COUNT(*) AS ratingCount, AVG(rating) as ratingAvg
FROM rating
GROUP BY movie_id
ORDER BY ratingAvg DESC;

SELECT movie_title, ratingAvg
FROM movie m JOIN topRatingMovies t ON m.movie_id = t.movie_ID
WHERE ratingCount > 10;
----

## Sqoop

Abreviatura de SQL + Hadoop. Nos permite importar y exportar datos de bases de datos relacionales y Hadoop.

Ejemplo: Importar una tabla a Hadoop

[source]
----
sqoop import --connect jdbc:mysql://localhost/movielens --table movies <1>
----
<1> Importa la tabla `movies` de la base de datos `movielens`.


Ejemplo: Importar una tabla a Hive

[source]
----
sqoop import --connect jdbc:mysql://localhost/movielens --table movies --hive-import <1>
----
<1> El parámetro `--hive-import` hace que la importación se lleve a cabo en Hive.

### Incremental imports

Incremental imports are performed by comparing the values in a check column against a reference value for the most recent import. For example, if the `--incremental` append argument was specified, along with `--check-column id` and `--last-value 100`, all rows with `id > 100` will be imported. 

Esto nos permite tener sincronizado a Hadoop con la base de datos.

### Intercambio de datos con MySQL

[source]
----
sqoop export --connect jdbc:mysql://db.example.com/foo \ 
--table exported_movies \ <1>
--export-dir /apps/hive/warehouse/movie <2>
----
<1> Tabla de destino en MySQL. Tiene que existir
<2> Path en Hadoop que contiene los datos a exportar de Hive

[NOTE]
====
La máquina virtual de Hortonworks incorpora un MySQL instalado con password `hadoop` para el usuario `root`.

Para la versión de HDP 2.6.5 y posteriores hay que configurar el pass del usuario `root` en MySQL.

image::MySQLResetPassword.png[]
====

#### Ejemplo: Importar datos a MySQL

[source]
----
mysql> create database movielens;
mysql> exit;

[maria_dev@sandbox ~]$ wget http://media.sundog-soft.com/hadoop/movielens.sql <1>

mysql> SET NAMES 'utf8'; <2>
mysql> SET CHARACTER SET 'utf8';

mysql> use movielens;
mysql> source movielens.sql <3>
mysql> show tables;
+---------------------+
| Tables_in_movielens |
+---------------------+
| genres              |
| genres_movies       |
| movies              |
| occupations         |
| ratings             |
| users               |
+---------------------+

mysql> GRANT ALL PRIVILEGES ON movielens.* TO ''@'localhost'; <4>

[maria_dev@sandbox ~]$ sqoop import --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table movies <5>

----
<1> Script que inicializa unas tablas de usuarios, películas y ratings.
<2> Configurar los juegos de caracteres
<3> Inicializar las tablas de la base de datos en MySQL
<4> Abrir todos los permisos en `localhost` para que podamos exportar desde Sqoop los datos de MySQL a Hadoop
<5> Importar los datos de MySQL a Hadoop. Los datos estarán en la vista de archivos del usuario `maria_dev`.

[NOTE]
====
Para importar los datos a Hive, primero deberíamos borrar el directorio `movies` de `maria_dev`, ya que la importación a Hive volverá a crearlo como paso intermedio para volcar los datos en él antes de importarlos a Hive.

[source]
----
[maria_dev@sandbox ~]$ sqoop import --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table movies --hive-import
----
====

#### Ejemplo: Exportar datos a MySQL

[source]
----
mysql> use movielens;
mysql> CREATE TABLE exported_movies (id INTEGER, title VARCHAR(255), releaseDate DATE); <1>

[maria_dev@sandbox ~]$ sqoop export --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table exported_movies --export-dir /apps/hive/warehouse/movies --input-fields-terminated-by '\0001' <2>
----
<1> La tabla de destino tiene que estar creada antes de hacer la exportación
<2> No olvidar indicar `--input-fields-terminated-by '\0001'`

## HBase

* Especializado en hacer operaciones muy rápidas a nivel de fila y ser muy escalable.
* Expone una API pero no ofrece un lenguaje de consulta.
* Es muy bueno para tratar con datos dispersos

Ejemplo: Crear una tabla de ratings agrupados por usuario.

Esto nos permite movernos rápidamente por las películas que ha evaluado un usuario.
Vamos a crear un script en Python que pueble esa tabla HBase.
Podremos crear un servicio que devuelva rápidamente los ratings de un userID.
El servicio atenderá peticiones HTTP para almacenar y recuperar datos a través de una API REST.

image::HBaseRatingColumnFamily.png[]

En la imagen la familia de columnas es _rating_ y las columnas son los identificadores de películas (50, 33 y 223).

* Expondremos el servicio a través de un puerto de la máquina virtual de HDP. En Virtual Box configuraremos `Settings | Network | Advanced | Port Forwarding. Añadiremos una regla denominada *HBase REST* que mapeará los puertos *8000*.
* Iniciar sesión en Ambari cono `admin` e iniciar el servicio HBase.
* Iniciar sesión SSH en la máquina virtual 
* Cambiar al usuario `root` y arrancar el servicio REST de HBase (`[root@sandbox maria_dev]# /usr/hdp/current/hbase-master/bin/hbase-daemon.sh start rest -p 8000 --infoport 8001`)
* Instalar con `pip` el módulo `starbase` para poder interactuar con HBase.
* Ejecutar el script Python que carga la tabla HBase y muestra un ejemplo de valoraciones de dos usuarios (`python ..../HBaseExamples.py`)


[source]
----
from starbase import Connection

c = Connection("127.0.0.1", "8000")

ratings = c.table('ratings')

if (ratings.exists()):
    print("Dropping existing ratings table\n")
    ratings.drop()

ratings.create('rating')

print("Parsing the ml-100k ratings data...\n")
ratingFile = open("ml-100k/u.data", "r")

batch = ratings.batch()

for line in ratingFile:
    (userID, movieID, rating, timestamp) = line.split()
    batch.update(userID, {'rating': {movieID: rating}})

ratingFile.close()

print ("Committing ratings data to HBase via REST service\n")
batch.commit(finalize=True)

print ("Get back ratings for some users...\n")
print ("Ratings for user ID 1:\n")
print (ratings.fetch("1"))
print ("Ratings for user ID 33:\n")
print (ratings.fetch("33"))
----

[NOTE]
====
Para finalizar HBase API REST

----
[root@sandbox maria_dev]# /usr/hdp/current/hbase-master/bin/hbase-daemon.sh stop
----
====

### Uso de Pig para la carga de datos en HBase

Usar Python para la carga de datos en HBase queda limitado porque sólo dispone del espacio que le proporciona el disco desde el equipo desde el que se lanza.

Ahora estamos interesados en tomar los datos directamente del cluster HDFS y guardarlos en una tabla HBase. Por tanto, cuando hablamos de Big Data necesitaremos de Pig o herramientas similares.

[NOTE]
====
`importtsv` es una herramienta que permite cargar datos HDFS en una tabla HBase.
====
Pasos:

. Crear la tabla HBase de destino
. Los datos a importar tienen un valor clave único en la primera columna
. Emplear `USING` para almacenar los datos en la tabla HBase

[NOTE]
====
Al ser HBase transaccional a nivel de fila, en una carga masiva de datos desde Pig se pueden lanzar gran cantidad de mappers en el momento de la carga de datos.
====


### Ejemplo: Cargar con Pig la tabla de usuarios en HBase

. Subir a HDFS el archivo de usuarios
. Crear la tabla HBase

[source, bash]
----
[maria_dev@sandbox ~]$ hbase shell

hbase(main):002:0> create 'users','userinfo'  <1>
----
<1> Crear una tabla `users` con una familia de columnas denominada `userinfo`

Este sería el script Pig que usaríamos. En el script vemos que el ID único ocupa la primera columna. También se ve el mapeo de las columnas del archivo HDFS contra la familia de columnas.

[source]
----
users = LOAD '/user/maria_dev/ml-100k/u.user' 
USING PigStorage('|') 
AS (userID:int, age:int, gender:chararray, occupation:chararray, zip:int);

STORE users INTO 'hbase://users' 
USING org.apache.pig.backend.hadoop.hbase.HBaseStorage (
'userinfo:age,userinfo:gender,userinfo:occupation,userinfo:zip');
----

Para ejecutarlo, una vez disponible este script en la máquina virtual ejecutarmos

[source]
----
[maria_dev@sandbox HadoopMaterials]$ pig hbase.pig 
----

Para ver los resultados de la carga de la tabla `users`, desde la shell de HBase (`hbase shell`):

[source]
----
hbase(main):004:0> scan users
...
 99                   column=userinfo:age, timestamp=1537097539475, value=20    <1>
 99                   column=userinfo:gender, timestamp=1537097539475, value=M  
 99                   column=userinfo:occupation, timestamp=1537097539475, value
                      =student                                                  
 99                   column=userinfo:zip, timestamp=1537097539475, value=63129 
943 row(s) in 2.4820 seconds 
----

<1> El resultado muestra la familia de columnas `userinfo` del usuario 99. La familia de columnas está formada por cuatro columnas: `age`, `gender`, `occupation` y `zip`.

Para eliminar la tabla primero hay que desactivarla con `disable`

## Cassandra

Es un DBMS distribuido que no tiene un punto de fallo único. A diferencia de HBase, no tiene nodo máster (el nodo que guarda qué datos guarda cada nodo). Está concebido para la disponibilidad.

* Su modelo de datos es similar al de HBase/BigTable
* Ofrece CQL, su lenguaje de consulta
* Cassandra proporciona consistencia eventual.

.El Teorema de CAP, consistencia eventual y consistencia ajustable
****
Siguiendo el Teorema de CAP, los problemas de Big Data necesitan Tolerancia a fallos (usamos HDFS y nuestro sistema es distribuido y tolerante a fallos de partida), por lo que tenemos que decidir entre Consistencia y Disponibilidad. Cassandra sacrifica Disponibilidad ofreciendo _consistencia eventual_ porque si envías un post a una red social no se cae el mundo si el resto de usuarios no ven el post hasta que no pasen 2 ó 3 segundos, lo que tarde en propagar el cambio por todo el cluster.

No obstante, en Cassandra se puede ajustar el número de nodos que tienen que confirmar la recepción de un cambio para que se dé por correcto. Es lo que se conoce como _Consistencia ajustable_
****

image::cap-theorem.png[]

* Podemos tener Cassandra operativo independientemente de Hadoop, por ejemplo, sirviendo a un sitio web. No obstante, se puede crear una copia para Hadoop para enriquecerlo con capacidades analíticas (Hive, Spark, ...)
* CQL tiene algunas limitaciones respecto a SQL
** No hay joins por lo que todos los datos tienen que estar desnormalizados
** Las consultas tienen que ser sobre la clave primaria. No se admiten índices secundarios
* Casos de uso de Cassandra y Spark
** Analítica de datos almacenados en Cassandra
** Transformar y guardar datos en Cassandra para uso transaccional


### Instalación de Cassandra

[CAUTION]
====
La máquina virtual quue estamos usando de Hortonworks contiene un sandbox en un contenedor Docker. Su versión de sistema operativo CentOS requiere Python 2.6. Sin embargo Cassandra necesita Python 2.7.
====

. Conexión por SSH a la máquina virtual a la cuenta de `maria_dev`
. Cambiar al usuario `root`.
. `# yum update`
. `# yum install scl-utils`
. `# yum install centos-release-scl-rh`
. `# yum install python27`
. `# scl enable python27 bash`
. Crear un archivo `/etc/yum.repos.d/datastax.repo` con este contenido
[source]
----
[datastax]
name = DataStax Repo for Apache Cassandra
baseurl = http://rpm.datastax.com/community
enabled = 1
gpgcheck = 0
----
. `# yum install dsc30`
. `# pip install cqlsh`
. `# service cassandra start`
. `# cqlsh --cqlversion="3.4.0"`
. `cqlsh> CREATE KEYSPACE movielens WITH replication = {'class':'SimpleStrategy', 'replication_factor':'1'} AND durable_writes = true;`
. `cqlsh> USE movielens;`
. `cqlsh:movielens> CREATE TABLE users(user_id int, age int, gender text, occupation text, zip text, PRIMARY KEY(user_id));`
. `cqlsh:movielens> DESCRIBE TABLE users;`
. `cqlsh:movielens> SELECT * FROM users;`


### Carga de datos en Cassandra desde Spark

[NOTE]
====
Usaremos datasets en el script por lo que usaremos Spark 2.0.
[source]
----
# export SPARK_MAJOR_VERSION=2
----
====

Archivo `CassandraSpark.py`

[source]
----
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql import functions

def parseInput(line):
    fields = line.split('|')
    return Row(user_id = int(fields[0]), age = int(fields[1]), gender = fields[2], occupation = fields[3], zip = fields[4])

if __name__ == "__main__":
    # Create a SparkSession
    spark = SparkSession.builder.appName("CassandraIntegration").config("spark.cassandra.connection.host", "127.0.0.1").getOrCreate()

    # Get the raw data
    lines = spark.sparkContext.textFile("hdfs:///user/maria_dev/ml-100k/u.user")
    # Convert it to a RDD of Row objects with (userID, age, gender, occupation, zip)
    users = lines.map(parseInput)
    # Convert that to a DataFrame
    usersDataset = spark.createDataFrame(users)

    # Write it into Cassandra
    usersDataset.write\
        .format("org.apache.spark.sql.cassandra")\
        .mode('append')\
        .options(table="users", keyspace="movielens")\
        .save()

    # Read it back from Cassandra into a new Dataframe
    readUsers = spark.read\
    .format("org.apache.spark.sql.cassandra")\
    .options(table="users", keyspace="movielens")\
    .load()

    readUsers.createOrReplaceTempView("users")

    sqlDF = spark.sql("SELECT * FROM users WHERE age < 20")
    sqlDF.show()

    # Stop the session
    spark.stop()
----

Para ejecutar el archivo:

[source]
----
spark-submit --packages datastax:spark-cassandra-connector:2.0.0-M2-s_2.11 CassandraSpark.py <1>
----
<1> Indicamos la versión 2.0.0 de del conector compatible con Spark 2.0 y Scala 2.11. Estos parámetros se ajustarán a la versión con la que estemos ejecutando.

## MongoDB

En el triángulo del teorema de CAP, MongoDB se sitúa en CP. Al manejar Big Data está comprometido con Tolerancia a fallos, y a la hora de elegir entre Consistencia y Disponibilidad, elige Consistencia. La Disponibilidad la deja entonces _comprometida_ teniendo un único máster, que es la base de datos a la que tienes que dirigirte si necesitas consistencia. Si el máster cae hay un periodo de no disponbilidad mientras otro master vuelve a estar disponible.

El éxito de MongoDB se debe en gran parte a la flexibildad de su modelo de datos. Entre sus principales características:

* Estructura de documentos flexible
* No es necesaria la creación de una _clave_ por parte nuestra.
* Permite la creación de índices, incluyendo índices fulltext e índices espaciales
* Permite la creación de shards sobre alguno de los índices definidos
* Dispone de un sistema de archivos propio para particionado denominado GridFS, que permite evitar el uso de Hadoop para manejar documentos muy grandes.
* Existe una base de datos principal y nodos secundarios conectados a la principal donce se replican los datos. Por tanto, así no va a servir directamente para Big data, ya que lo que tenemos son copias de una base de datos monolítica.

[NOTE]
====
MongoDB ofrece un conector SQL que traduce consultas SQL a consultas en MongoDB. Es útil para integrar MongoDB con herramientas de BI de las usadas con bases de datos relacionales (p.e Tableau). https://docs.mongodb.com/bi-connector/master/supported-operations/
====

### Detalles a tener en cuenta con los Replica Sets

* Es necesario contar con una mayoría entre un número impar de servidores. Por tanto, necesitamos al menos 3 servidores.
* Las aplicaciones tienen que tener conocimiento de varios servidores del replica set para poder llegar al principal
* Las réplicas ofrecen durabilidad. No es conveniente leer directamente de los secundarios.
* La base de datos queda en modo lectura mientras se elige un nuevo primario.

### Sharding 

Para conseguir Big data con MongoDB necesitamos _sharding_. El _sharding_ se combina con los replica sets de forma que cada replica set es responsable de un rango de valores de acuerdo a un índice definido previamente. El servidor de aplicaciones ejecuta un proceso denominado `mongos` que interactúa con los _servidores de configuración_ que son los que saben cómo está definido el _sharding_ y en qué servidores están los datos que necesitamos.

[NOTE]
====
El _sharding_ también tiene sus caprichos:

* El autosharding puede provocar lo que se denomina _split storms_, en el que los procesos `mongos` se reinician continuamente porque intenta rebalancear los _shards_ pero no consigue hacerlo a tiempo y vuelve a comenzar a rebalancear.
* Hay que tener 3 servidores de configuración. Si uno de ellos cae, la base de datos también cae.
====

### Instalación de MongoDB

Es posible instalar MongoDB como un servicio de Ambari

[source]
----
[root@sandbox maria_dev]# cd /var/lib/ambari-server/resources/stacks/HDP/2.5/services/
[root@sandbox services]# ls
ACCUMULO  HBASE  KERBEROS  OOZIE       SLIDER  stack_advisor.py   TEZ
ATLAS     HDFS   KNOX      PIG         SPARK   stack_advisor.pyc  YARN
FALCON    HIVE   MAHOUT    RANGER      SPARK2  stack_advisor.pyo  ZEPPELIN
FLUME     KAFKA  NIFI      RANGER_KMS  SQOOP   STORM              ZOOKEEPER

[root@sandbox services]# git clone http://github.com/nikunjness/mongo-ambari.git

[root@sandbox services]# service ambari restart
----

A continuación, desde Ambari y conectados como `admin`, añadimos el servicio MongoDB en el desplegable de `Actions`

pip install pymongo


## Links

* Máquina virtual de Hortoworks: https://es.hortonworks.com/downloads/#sandbox
* Datos de MovieLens: https://grouplens.org/datasets/movielens/100k/
* Material: https://sundog-education.com/hadoop-materials/
